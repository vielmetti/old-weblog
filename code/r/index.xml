<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on tracker</title>
    <link>http://vielmetti.github.io/code/r/</link>
    <description>Recent content in R on tracker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Mar 2015 11:14:50 -0400</lastBuildDate>
    <atom:link href="http://vielmetti.github.io/code/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Cleaning polygons in R and Shapely</title>
      <link>http://vielmetti.github.io/post/2015/2015-03-04-cleaning-polygons-in-r-and-shapely/</link>
      <pubDate>Wed, 04 Mar 2015 11:14:50 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-03-04-cleaning-polygons-in-r-and-shapely/</guid>
      <description>&lt;p&gt;The project of the week has been an effort to generate a set of maps - 2935 of them, in fact. The source data is from the Department of Energy, listing a set of utility companies and the counties they cover; the goal is to have a boundary map generated for each utility company. I know it won&#39;t be a perfect boundary (most utility companies have at least one county that they only provide service to part of), but as a start it&#39;s a pretty good one.&lt;/p&gt;

&lt;p&gt;The DOE data is based on their Form EIA-861, and I&#39;m working from the 2013 data set. See &lt;a href=&#34;http://www.eia.gov/electricity/data/eia861/&#34;&gt;Electric power sales, revenue, and energy efficiency Form EIA-861 detailed data files&lt;/a&gt; if you want to get your own files. It&#39;s provided as Excel (XLSX) files, so you&#39;ll need something compatible to read them; I ended up using &lt;a href=&#34;http://csvkit.readthedocs.org/en/latest/scripts/in2csv.html&#34;&gt;in2csv from csvkit&lt;/a&gt; as the first part of the pipeline to break these apart.&lt;/p&gt;

&lt;p&gt;You get lines from the provided file that look like this:&lt;/p&gt;

&lt;pre&gt;
Data Year,Utility Number,Utility Name,State,County
2013,5109,DTE Electric Company,MI,Shiawassee
2013,5109,DTE Electric Company,MI,St Clair
2013,5109,DTE Electric Company,MI,Tuscola
2013,5109,DTE Electric Company,MI,Washtenaw
2013,5109,DTE Electric Company,MI,Wayne
&lt;/pre&gt;

&lt;p&gt;The next challenge is to pull a map for each county served. To do this, convert the (State,County) list to a FIPS code, using a table like this one from ORNL called &lt;a href=&#34;http://cta.ornl.gov/transnet/CoFIPS00.txt&#34;&gt;CoFIPS00.txt&lt;/a&gt;. (MI,Washtenaw) turns into 26161. Feed a list of FIPS codes into the Census Reporter geo API, and out comes GeoJSON files, like so: &lt;a href=&#34;http://api.censusreporter.org/1.0/geo/show/tiger2013?geo_ids=05000US26103,05000US26003&#34;&gt;GeoJSON for Marquette and Alger Counties, Michigan&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;We have a list of counties for each utility, so at minimum we&#39;re set right there. But we need to get 2935 of them, and it&#39;s kind of slow to wait for each one to return before fetching the next one. That motivates the use of &lt;a href=&#34;http://www.gnu.org/software/parallel/&#34;&gt;GNU Parallel&lt;/a&gt;, a shell tool for executing jobs in parallel. On my four core MacBook Air I ran 32 jobs in parallel, and 2935 fetches (using &#34;curl&#34;) took 287 seconds with the median elapsed time for each job at about 3 seconds - about a 31x speedup vs doing things serially.&lt;/p&gt;

&lt;p&gt;However, the GeoJSON provided is really one shape per county, and we&#39;re looking for a complete service area in a single shape, so we need to merge the result together. Here a useful tool is the &#34;R&#34; stats package, particularly the &#34;rgeos&#34;, &#34;rgdal&#34;, and &#34;maptools&#34; packages. &lt;/p&gt;

&lt;p&gt;The first step is the &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgdal/docs/ogrInfo&#34;&gt;readOGR() function from the rgdal library&lt;/a&gt;. It takes a variety of input map formats including GeoJSON and shape files and pulls them into a Spatial vector object in R. rgdal is an R interface to Frank Warmerdam&#39;s &lt;a href=&#34;http://www.gdal.org/&#34;&gt;Geospatial Data Abstraction Library&lt;/a&gt;, and it handles both raster and vector geospatial data formats.&lt;/p&gt;

&lt;p&gt;Next, use the &lt;a href=&#34;http://www.inside-r.org/packages/cran/maptools/docs/unionSpatialPolygons&#34;&gt;unionSpatialPolygons() function from the maptools library&lt;/a&gt; to merge the county polygons into a single polygon. The docs say that this is a wrapper around the &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgeos/docs/gUnion&#34;&gt;gUnion() function from the rgeos library&lt;/a&gt;. rgeos comes from the Google Summer of Code 2010, and this &lt;a href=&#34;https://gsoc2010r.wordpress.com/2010/06/10/rgeos-introduction/&#34;&gt;introduction to the project&lt;/a&gt; explains the motivation. rgeos provides an R interface to &lt;a href=&#34;http://trac.osgeo.org/geos/&#34;&gt;GEOS&lt;/a&gt;, which is in turn a C++ port of &lt;a href=&#34;http://tsusiatsoftware.net/jts/main.html&#34;&gt;JTS&lt;/a&gt;, the Java Topology Suite.&lt;/p&gt;

&lt;p&gt;After you run the unionSpatialPolygons() operation, you will inevitably find that there are tiny holes in your unioned map. Geographic boundaries are complex, and as a result there are often little areas that don&#39;t quite touch, leaving little bits of land that aren&#39;t included anywhere. This algorithm from stackoverflow, &lt;a href=&#34;http://stackoverflow.com/questions/12663263/dissolve-holes-in-polygon-in-r&#34;&gt;Dissolve holes in polygon in R&lt;/a&gt;, takes care of the issue. &lt;/p&gt;

&lt;pre&gt;
G.rings = Filter(function(f){f@ringDir==1},G.union@polygons[[1]]@Polygons)
G.bounds = SpatialPolygons(list(Polygons(G.rings,ID=1)))
&lt;/pre&gt;

&lt;p&gt;As the Polygon documentation note for ringDir: &#34;the ring direction of the ring (polygon) coordinates, holes are expected to be anti-clockwise&#34;&lt;/p&gt;

&lt;p&gt;Having done this cleanup work, we return a GeoJSON file with the &lt;a href=&#34;http://www.rdocumentation.org/packages/leafletR/functions/toGeoJSON&#34;&gt;toGeoJSON() function from the leafletR package&lt;/a&gt;. Now I probably don&#39;t need to pull in all of leafletR; there&#39;s a &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgdal/docs/writeOGR&#34;&gt;writeOGR() function in the rgdal package&lt;/a&gt; that should also do the same work. The aside, though, is that &lt;a href=&#34;https://github.com/chgrl/leafletR&#34;&gt;leafletR&lt;/a&gt; creates nice maps in R using the &lt;a href=&#34;http://leafletjs.com/&#34;&gt;Leaflet&lt;/a&gt; Javascript library.&lt;/p&gt;

&lt;p&gt;With all that done, we have a new GeoJSON file that&#39;s a cleaned up merge of multiple county outlines done in R. However, the process isn&#39;t quite done. Some of the merged files don&#39;t have holes, but they do have &#34;slivers&#34;, little angular bits of geometry from where two counties don&#39;t quite touch at their corners. I didn&#39;t find an algorithm to unsliver in R, but this &lt;a href=&#34;http://gis.stackexchange.com/questions/120286/removing-small-polygons-gaps-in-a-shapely-polygon&#34;&gt;Removing small polygons gaps in a Shapely polygon&lt;/a&gt; did the trick. &lt;/p&gt;

&lt;pre&gt;
# Here&#39;s the algorithm
fx = poly.buffer(eps, 1, join_style=JOIN_STYLE.mitre).buffer(-eps, 1, join_style=JOIN_STYLE.mitre)
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://toblerity.org/shapely/manual.html&#34;&gt;Shapely&lt;/a&gt; from Sean Gilles is a &#34;Python package for set-theoretic analysis and manipulation of planar features using (via Pythonâ€™s ctypes module) functions from the well known and widely deployed GEOS library.&#34; So we&#39;re back to GEOS, which is also supported by R...which means I suspect this algorithm translates directly back into R by proper invocation of the equivalent &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgeos/docs/gBuffer&#34;&gt;gBuffer() function from the rgeos package&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;No matter, I installed Shapely. It&#39;s pretty fast, and if I had time to refine this operation I&#39;d try to port all of the R code back into python using Shapely to see if I could get a meaningful speedup improvement.&lt;/p&gt;

&lt;p&gt;All this description! Here&#39;s the code.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/vielmetti/13f6feae14111ff1c148&#34;&gt;unsliver.py&lt;/a&gt; - in Python with the Shapely library&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/450495defeff6e4d1440&#34;&gt;mergeGeoJSON.Rscript&lt;/a&gt; - in R with rgeos, sp, rgdal, maptools, and leafletR&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And here&#39;s a picture:&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/vielmetti/2a6feadf268ca2b686e7.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Thanks go to Joe Germuska (for Census Reporter and csvkit), Markus Spath (for R help), Matt Hampel (for mapping help), Jacob Wasserman (for Shapely help), Stan Gregg (for outage mapping), Mohan Kartha (for GNU Parallel and AWS help), and anyone else who I missed along the way.                                          &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Maps with Leaflet and LeafletR</title>
      <link>http://vielmetti.github.io/post/2015/2015-03-02-maps-with-leaflet-and-leafletr/</link>
      <pubDate>Mon, 02 Mar 2015 01:09:35 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-03-02-maps-with-leaflet-and-leafletr/</guid>
      <description>&lt;pre&gt;
&gt; style &lt;- styleGrad(prop=&#34;score&#34;, breaks=seq(50,100,by=10) ,style.val=rev(rainbow(5)), leg=&#34;score&#34;)
&gt; leaflet(data=&#34;/Users/emv/tmp/N.geojson&#34;,dest=tempdir(),popup=&#34;*&#34;,style=style)
&lt;/pre&gt;

&lt;p&gt;So, progress.&lt;/p&gt;

&lt;p&gt;One process, written in python, takes an Excel file provided by the county and creates a GeoJSON file from it. Addresses in the file are geocoded. The original file has a set of restaurant inspection reports and three kinds of inspection problems found; I add a fourth &#34;score&#34; variable that&#39;s derived in some way from those three.&lt;/p&gt;

&lt;p&gt;A second process (code above) is in R using the LeafletR graphing package. It in turn takes the GeoJSON file as originally provided, and writes an HTML file that reads it using the Leaflet maps package.&lt;/p&gt;

&lt;p&gt;A screenshot of the result is below:&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;asset-img-link&#34;  href=&#34;http://vielmetti.typepad.com/.a/6a00d8341c4f1a53ef01b7c7572bc9970b-pi&#34;&gt;&lt;img class=&#34;asset  asset-image at-xid-6a00d8341c4f1a53ef01b7c7572bc9970b img-responsive&#34; style=&#34;width: 600px; display: block; margin-left: auto; margin-right: auto;&#34; alt=&#34;Screen Shot 2015-03-02 at 12.56.42 AM&#34; title=&#34;Screen Shot 2015-03-02 at 12.56.42 AM&#34; src=&#34;http://vielmetti.typepad.com/.a/6a00d8341c4f1a53ef01b7c7572bc9970b-600wi&#34; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Some big caveats.&lt;/p&gt;

&lt;p&gt;My color palette is all wrong - I need to think that through. &lt;/p&gt;

&lt;p&gt;This process is based on a single monthly Excel file, and should be updated to include all recent inspections and not just a single month.&lt;/p&gt;

&lt;p&gt;The map needs labels of some sort. (How they will look when everything is smushed together, don&#39;t know.)&lt;/p&gt;

&lt;p&gt;It would be nice to have HTML links to the Arborwiki page for each restaurant, and to a deep link for the inspections itself.&lt;/p&gt;

&lt;p&gt;The geocoder is a bit slow, and I should cache previous results so that it goes faster in future months.&lt;/p&gt;

&lt;p&gt;RESTAURANT NAMES ARE IN ALL CAPS AND THAT&#39;S SHOUTING.&lt;/p&gt;

&lt;p&gt;Leaflet can do all sorts of things and I&#39;ve hardly scratched the surface.&lt;/p&gt;

&lt;p&gt;I probably could do this all in R - or all in Python - but I&#39;m trying to spin up on both systems &amp;amp; thus really want to use each for what they are good at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>