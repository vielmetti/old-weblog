<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vacuum weblog from Edward Vielmetti</title>
    <link>http://vielmetti.github.io/code/mongodb/index.xml</link>
    <description>Recent content on Vacuum weblog from Edward Vielmetti</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://vielmetti.github.io/code/mongodb/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>First pass at MongoDB</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-26-first-pass-at-mongodb/</link>
      <pubDate>Wed, 26 Aug 2015 00:40:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-26-first-pass-at-mongodb/</guid>
      <description>&lt;p&gt;Somehow I never learned SQL along the way. At various times I have
learned enough of the database lingo (inner join, outer join etc) to
know where I missing things, and I have a lot of experience with common
Unix tools (grep, sed, awk) and some newfangled tools (jq, csvkit) to
be able to turn a big pile of data into a smaller pile of data. But
I don&amp;rsquo;t have database chops per se right now, and sometimes that&amp;rsquo;s a
limiting factor on projects when the data sets are too big.&lt;/p&gt;

&lt;p&gt;JSON formatted data seems to have won the data serialization wars, so
that at least transforms the problem on input; with jq (now in release 1.5)
you can take a stream of JSON formatted data that&amp;rsquo;s almost what you want
and mangle it until it&amp;rsquo;s really what you want. Lots of web APIs spit out
some flavor of JSON, and even if it&amp;rsquo;s not precisely formatted to be ready
for input into something else, at least there&amp;rsquo;s a straightforward way to
munge it into something better.&lt;/p&gt;

&lt;p&gt;That leaves the task of dumping these JSON blobs into a database, in
such a way that you can relatively easily get something back out of the
database that answers what you are really trying to answer again. Here
the world splits into two pieces: the SQL databases that have been taught
how to deal with JSON (specifically PostgreSQL, as of version 9.3), and the
so-called NoSQL databases (specifically MongoDB).&lt;/p&gt;

&lt;p&gt;Even Mongo knows that &lt;a href=&#34;http://blog.mongodb.org/post/62899600960/scaling-advice-from-mongohq&#34;&gt;scaling MongoDB is hard&lt;/a&gt;, though
they frame it as the first 100 gigabytes of data stored in the system is
easy. Fortunately, the first set of tasks I have to throw at this particular
problem space are relatively small, databases measured in the 10s of megabytes.
Pretty much anything will work - even grep - on this size data. The
challenge becomes more of a how fast can you get something real running
that has relatively simple queries, relatively performant results, and
that can run on inexpensive hardware.&lt;/p&gt;

&lt;p&gt;So I learned just enough Mongo to handle one tiny task: connect to an API,
download some data periodically, clean it up enough so that it&amp;rsquo;s regular,
and store it away; and then construct queries that are conceptually simple
but that answer real questions. From there, do the ad hoc queries that prove
to yourself that you have the right data, and then bootstrap some web
app on top of all that to show off what you&amp;rsquo;ve done.&lt;/p&gt;

&lt;p&gt;The results here accomplish a relatively complex task with a handful of
lines of code. An API from a local agency is queried that has real time data.
That data gets cleaned up and standardized and then inserted into a Mongo
database. It&amp;rsquo;s all of 6 lines of code.&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/vielmetti/85d5fba07b98e552c912.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
  </channel>
</rss>