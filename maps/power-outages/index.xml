<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Power Outages on tracker</title>
    <link>http://localhost:1313/maps/power-outages/</link>
    <description>Recent content in Power Outages on tracker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Mar 2015 11:14:50 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/maps/power-outages/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Cleaning polygons in R and Shapely</title>
      <link>http://localhost:1313/post/2015/2015-03-04-cleaning-polygons-in-r-and-shapely/</link>
      <pubDate>Wed, 04 Mar 2015 11:14:50 -0400</pubDate>
      
      <guid>http://localhost:1313/post/2015/2015-03-04-cleaning-polygons-in-r-and-shapely/</guid>
      <description>&lt;p&gt;The project of the week has been an effort to generate a set of maps - 2935 of them, in fact. The source data is from the Department of Energy, listing a set of utility companies and the counties they cover; the goal is to have a boundary map generated for each utility company. I know it won&#39;t be a perfect boundary (most utility companies have at least one county that they only provide service to part of), but as a start it&#39;s a pretty good one.&lt;/p&gt;

&lt;p&gt;The DOE data is based on their Form EIA-861, and I&#39;m working from the 2013 data set. See &lt;a href=&#34;http://www.eia.gov/electricity/data/eia861/&#34;&gt;Electric power sales, revenue, and energy efficiency Form EIA-861 detailed data files&lt;/a&gt; if you want to get your own files. It&#39;s provided as Excel (XLSX) files, so you&#39;ll need something compatible to read them; I ended up using &lt;a href=&#34;http://csvkit.readthedocs.org/en/latest/scripts/in2csv.html&#34;&gt;in2csv from csvkit&lt;/a&gt; as the first part of the pipeline to break these apart.&lt;/p&gt;

&lt;p&gt;You get lines from the provided file that look like this:&lt;/p&gt;

&lt;pre&gt;
Data Year,Utility Number,Utility Name,State,County
2013,5109,DTE Electric Company,MI,Shiawassee
2013,5109,DTE Electric Company,MI,St Clair
2013,5109,DTE Electric Company,MI,Tuscola
2013,5109,DTE Electric Company,MI,Washtenaw
2013,5109,DTE Electric Company,MI,Wayne
&lt;/pre&gt;

&lt;p&gt;The next challenge is to pull a map for each county served. To do this, convert the (State,County) list to a FIPS code, using a table like this one from ORNL called &lt;a href=&#34;http://cta.ornl.gov/transnet/CoFIPS00.txt&#34;&gt;CoFIPS00.txt&lt;/a&gt;. (MI,Washtenaw) turns into 26161. Feed a list of FIPS codes into the Census Reporter geo API, and out comes GeoJSON files, like so: &lt;a href=&#34;http://api.censusreporter.org/1.0/geo/show/tiger2013?geo_ids=05000US26103,05000US26003&#34;&gt;GeoJSON for Marquette and Alger Counties, Michigan&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;We have a list of counties for each utility, so at minimum we&#39;re set right there. But we need to get 2935 of them, and it&#39;s kind of slow to wait for each one to return before fetching the next one. That motivates the use of &lt;a href=&#34;http://www.gnu.org/software/parallel/&#34;&gt;GNU Parallel&lt;/a&gt;, a shell tool for executing jobs in parallel. On my four core MacBook Air I ran 32 jobs in parallel, and 2935 fetches (using &#34;curl&#34;) took 287 seconds with the median elapsed time for each job at about 3 seconds - about a 31x speedup vs doing things serially.&lt;/p&gt;

&lt;p&gt;However, the GeoJSON provided is really one shape per county, and we&#39;re looking for a complete service area in a single shape, so we need to merge the result together. Here a useful tool is the &#34;R&#34; stats package, particularly the &#34;rgeos&#34;, &#34;rgdal&#34;, and &#34;maptools&#34; packages. &lt;/p&gt;

&lt;p&gt;The first step is the &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgdal/docs/ogrInfo&#34;&gt;readOGR() function from the rgdal library&lt;/a&gt;. It takes a variety of input map formats including GeoJSON and shape files and pulls them into a Spatial vector object in R. rgdal is an R interface to Frank Warmerdam&#39;s &lt;a href=&#34;http://www.gdal.org/&#34;&gt;Geospatial Data Abstraction Library&lt;/a&gt;, and it handles both raster and vector geospatial data formats.&lt;/p&gt;

&lt;p&gt;Next, use the &lt;a href=&#34;http://www.inside-r.org/packages/cran/maptools/docs/unionSpatialPolygons&#34;&gt;unionSpatialPolygons() function from the maptools library&lt;/a&gt; to merge the county polygons into a single polygon. The docs say that this is a wrapper around the &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgeos/docs/gUnion&#34;&gt;gUnion() function from the rgeos library&lt;/a&gt;. rgeos comes from the Google Summer of Code 2010, and this &lt;a href=&#34;https://gsoc2010r.wordpress.com/2010/06/10/rgeos-introduction/&#34;&gt;introduction to the project&lt;/a&gt; explains the motivation. rgeos provides an R interface to &lt;a href=&#34;http://trac.osgeo.org/geos/&#34;&gt;GEOS&lt;/a&gt;, which is in turn a C++ port of &lt;a href=&#34;http://tsusiatsoftware.net/jts/main.html&#34;&gt;JTS&lt;/a&gt;, the Java Topology Suite.&lt;/p&gt;

&lt;p&gt;After you run the unionSpatialPolygons() operation, you will inevitably find that there are tiny holes in your unioned map. Geographic boundaries are complex, and as a result there are often little areas that don&#39;t quite touch, leaving little bits of land that aren&#39;t included anywhere. This algorithm from stackoverflow, &lt;a href=&#34;http://stackoverflow.com/questions/12663263/dissolve-holes-in-polygon-in-r&#34;&gt;Dissolve holes in polygon in R&lt;/a&gt;, takes care of the issue. &lt;/p&gt;

&lt;pre&gt;
G.rings = Filter(function(f){f@ringDir==1},G.union@polygons[[1]]@Polygons)
G.bounds = SpatialPolygons(list(Polygons(G.rings,ID=1)))
&lt;/pre&gt;

&lt;p&gt;As the Polygon documentation note for ringDir: &#34;the ring direction of the ring (polygon) coordinates, holes are expected to be anti-clockwise&#34;&lt;/p&gt;

&lt;p&gt;Having done this cleanup work, we return a GeoJSON file with the &lt;a href=&#34;http://www.rdocumentation.org/packages/leafletR/functions/toGeoJSON&#34;&gt;toGeoJSON() function from the leafletR package&lt;/a&gt;. Now I probably don&#39;t need to pull in all of leafletR; there&#39;s a &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgdal/docs/writeOGR&#34;&gt;writeOGR() function in the rgdal package&lt;/a&gt; that should also do the same work. The aside, though, is that &lt;a href=&#34;https://github.com/chgrl/leafletR&#34;&gt;leafletR&lt;/a&gt; creates nice maps in R using the &lt;a href=&#34;http://leafletjs.com/&#34;&gt;Leaflet&lt;/a&gt; Javascript library.&lt;/p&gt;

&lt;p&gt;With all that done, we have a new GeoJSON file that&#39;s a cleaned up merge of multiple county outlines done in R. However, the process isn&#39;t quite done. Some of the merged files don&#39;t have holes, but they do have &#34;slivers&#34;, little angular bits of geometry from where two counties don&#39;t quite touch at their corners. I didn&#39;t find an algorithm to unsliver in R, but this &lt;a href=&#34;http://gis.stackexchange.com/questions/120286/removing-small-polygons-gaps-in-a-shapely-polygon&#34;&gt;Removing small polygons gaps in a Shapely polygon&lt;/a&gt; did the trick. &lt;/p&gt;

&lt;pre&gt;
# Here&#39;s the algorithm
fx = poly.buffer(eps, 1, join_style=JOIN_STYLE.mitre).buffer(-eps, 1, join_style=JOIN_STYLE.mitre)
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://toblerity.org/shapely/manual.html&#34;&gt;Shapely&lt;/a&gt; from Sean Gilles is a &#34;Python package for set-theoretic analysis and manipulation of planar features using (via Pythonâ€™s ctypes module) functions from the well known and widely deployed GEOS library.&#34; So we&#39;re back to GEOS, which is also supported by R...which means I suspect this algorithm translates directly back into R by proper invocation of the equivalent &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgeos/docs/gBuffer&#34;&gt;gBuffer() function from the rgeos package&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;No matter, I installed Shapely. It&#39;s pretty fast, and if I had time to refine this operation I&#39;d try to port all of the R code back into python using Shapely to see if I could get a meaningful speedup improvement.&lt;/p&gt;

&lt;p&gt;All this description! Here&#39;s the code.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/vielmetti/13f6feae14111ff1c148&#34;&gt;unsliver.py&lt;/a&gt; - in Python with the Shapely library&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/450495defeff6e4d1440&#34;&gt;mergeGeoJSON.Rscript&lt;/a&gt; - in R with rgeos, sp, rgdal, maptools, and leafletR&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And here&#39;s a picture:&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/vielmetti/2a6feadf268ca2b686e7.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Thanks go to Joe Germuska (for Census Reporter and csvkit), Markus Spath (for R help), Matt Hampel (for mapping help), Jacob Wasserman (for Shapely help), Stan Gregg (for outage mapping), Mohan Kartha (for GNU Parallel and AWS help), and anyone else who I missed along the way.                                          &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Power outage maps</title>
      <link>http://localhost:1313/post/2008/2008-09-03-power-outage-ma/</link>
      <pubDate>Wed, 03 Sep 2008 16:26:16 +0000</pubDate>
      
      <guid>http://localhost:1313/post/2008/2008-09-03-power-outage-ma/</guid>
      <description>&lt;p&gt;Whenever the power goes out, there&#39;s a burst of search traffic by people to try to figure out what is going on and when the lights will be back on.&lt;/p&gt;

&lt;p&gt;Lots of utility companies have outage management systems which provide some widely varied amount of information about what&#39;s going on - there&#39;s a bunch of vendors, no obvious standards being used, and even file formats and page styles are all over the place.&amp;nbsp; With hurricanes on their way, and dozens of different electric companies dealing with possible problems, it looked like a mess.&amp;nbsp; So I made a map.&lt;/p&gt;

&lt;iframe width=&#34;425&#34; scrolling=&#34;no&#34; height=&#34;350&#34; frameborder=&#34;0&#34; marginheight=&#34;0&#34; marginwidth=&#34;0&#34; src=&#34;http://maps.google.com/maps/ms?ie=UTF8&amp;amp;hl=en&amp;amp;s=AARTsJp0kTd9LVAlt2TXywCOVrI94rIX_g&amp;amp;msa=0&amp;amp;msid=113932165856461159137.000455fd4a9e930f6b943&amp;amp;ll=36.668419,-83.935547&amp;amp;spn=24.587157,37.353516&amp;amp;z=4&amp;amp;output=embed&#34;&gt; &lt;/iframe&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&#34;http://maps.google.com/maps/ms?ie=UTF8&amp;amp;hl=en&amp;amp;msa=0&amp;amp;msid=113932165856461159137.000455fd4a9e930f6b943&amp;amp;ll=36.668419,-83.935547&amp;amp;spn=24.587157,37.353516&amp;amp;z=4&amp;amp;source=embed&#34; style=&#34;color: rgb(0, 0, 255); text-align: left;&#34;&gt;View Larger Map&lt;/a&gt;&lt;/small&gt;

&lt;/p&gt;

&lt;p&gt;This map (incomplete now; notably missing the west coast and new england) has regions corresponding to power company service areas, more or less, and links from those regions direct to the outage reporting page.&amp;nbsp; It&#39;s a hack - you&#39;d love something lovely which really put all of the live data on one page - but you live with the infrastructure you have, not the infrastructure you want to have.&lt;/p&gt;

&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;As I said a bunch of vendors are working on these sorts of tools; here&#39;s a review of sorts.&lt;/p&gt;

&lt;p&gt;The shiniest and nicest so far appears to be &lt;a href=&#34;http://www.ifactorconsulting.com&#34;&gt;iFactor&lt;/a&gt;, which handles mapping for Con Edison (NYC), Progress Energy (Fla), Nebraska Public Power District and Entergy (New Orleans). The New Orleans system has had a tremendous workout with Gustav, and at some point the crush of queries made them take it offline; it went back online with limited information (ie. the utility doesn&#39;t know when things are going to be online again so that part of the system is disabled).&amp;nbsp; They are using Microsoft Virtual Earth for their base maps.&lt;/p&gt;






</description>
    </item>
    
  </channel>
</rss>