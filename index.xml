<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vacuum weblog from Edward Vielmetti</title>
    <link>http://vielmetti.github.io/</link>
    <description>Recent content on Vacuum weblog from Edward Vielmetti</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Oct 2015 13:00:00 -0400</lastBuildDate>
    <atom:link href="http://vielmetti.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Over the top networks</title>
      <link>http://vielmetti.github.io/post/2015/2015-10-21-over-the-top-networks/</link>
      <pubDate>Wed, 21 Oct 2015 13:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-10-21-over-the-top-networks/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m giving a talk on Over The Top Networks at the Radical/Networks
conference in Brooklyn this weekend, October 24-25 2015.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t know exactly what I&amp;rsquo;m going to say in what order yet, but&amp;hellip;
the best way to predict that is to write down some notes. So here we go.&lt;/p&gt;

&lt;p&gt;The premise, briefly stated, is that most successful networks run
&amp;ldquo;over the top&amp;rdquo; of existing working networks, and that you can learn
a lot from what goes wrong along the way when the underlying network
fights back.&lt;/p&gt;

&lt;p&gt;Lots of examples, here&amp;rsquo;s just some of them.&lt;/p&gt;

&lt;p&gt;&amp;ndash;&lt;/p&gt;

&lt;p&gt;APRS is a worldwide location sharing system that runs over amateur packet
radio. It runs &amp;ldquo;over the top&amp;rdquo; of VHF radios, and sends out location
information for participating nodes to digital repeaters (&amp;ldquo;digipeaters&amp;rdquo;) that
relay the signal to others in the participating mesh. It&amp;rsquo;s designed
initially to be completely free of external infrastructure dependencies,
except for some good source of location information provided by GPS.&lt;/p&gt;

&lt;p&gt;This network worked fine at first, but as it got more popular, the network
got more congested. A single transmission from a moving vehicle would trigger
two or three rounds of repeaters resending the message, and in places
where the network was dense with stations there would be collisions on
the single radio channel. To compensate, some stations would set their
initial broadcast to repeat four or five times, increasing their own
chance of getting heard at the expense of more congestion network-wide.&lt;/p&gt;

&lt;p&gt;The solution ended up being fairly simple, once the Internet got to be
reliable enough. An internet gateway (&amp;ldquo;I-Gate&amp;rdquo;) was set up to capture
location information shared by radios, so that instead of transmitting this
data via collision-prone VHF radios, it went over the Internet. A much
small number of internet-enabled gateways were able to handle the load,
and it allowed stations that only had Internet connections to participate
in the network by sending their location reports directly to the I-Gate.&lt;/p&gt;

&lt;p&gt;There are all sorts of radio lessons to be learned from the saga of APRS.
Stations sharing a single low-speed narrow band radio channel have to
work extra hard not to collide with each other, and the &amp;ldquo;hidden terminal&amp;rdquo;
problem (where a station interferes with another station that it can&amp;rsquo;t hear,
but that can hear it) ensures that network density leads to more problems.&lt;/p&gt;

&lt;p&gt;&amp;ndash;&lt;/p&gt;

&lt;p&gt;Usenet News started in 1979 in North Carolina, connecting computers in
the Research Triangle between Duke and the University of North Carolina.
It grew rapidly in the 1980s, taking advantage of whatever networks
were available to it. The relatively simple Unix-to-Unix Copy approach
to networking meant that virtually any data network that could be coaxed
into sending a data stream unmodified over a circuit could be put to use.&lt;/p&gt;

&lt;p&gt;Usenet ran over the top of dialup phone lines in its early times,
and this posed a problem: long distance calls cost money. Several
methods were used to great effect to get away from this financial
limitation on network growth.&lt;/p&gt;

&lt;p&gt;One very simple way to bypass telephone bills is to have the telephone
company call you. Researchers at Bell Labs, the home of Unix, had a great
interest in seeing the network succeed, and thus they routinely set up
their systems to make large numbers of outbound telephone calls to poll
remote stations for mail and news. The costs of toll long distance
telephone service were hidden inside the phone company&amp;rsquo;s own phone bills,
as well as the phone bills of several other large computer companies
where agile systems staff were able to avoid the attention of bean
counters (who would probably not have approved of spending corporate
dollars to retransmit rec.pets.cats).&lt;/p&gt;

&lt;p&gt;A parallel approach was to reuse dialup networks originally designed
for terminals to connect to mainframe computers and repurpose them for
machine-to-machine use. In Michigan, the statewide MERIT computer network
was designed to connect research and education computing across the state.
Clever systems programmers (including yours truly) realized that it could
be also used to support slow but functional UUCP connections, and that
those links would not incur any per-hour or per-kilobyte charges. Thus
a bulletin board system in Michigan&amp;rsquo;s remote Upper Peninsula could get
free worldwide email and Usenet news feeds by befriending the right people
in Ann Arbor and adapting the particular arcane configuration.&lt;/p&gt;

&lt;p&gt;As Usenet grew, the demand for bandwidth started to outstrip the
capacity of dialup telephone circuits. This led to efforts to use and
reuse the new source of &amp;ldquo;free&amp;rdquo; long-distance data services, namely the
evolving Internet of its day. By the time the National Science Foundation
had christened its super-fast, 1.5 megabit per second network, Usenet
News admins were ready with the NNTP protocol which could take best
advantage of the new network.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s estimated that over the 10 year lifespan of the NSFnet that public
sources contributed over $2 billion to the network - $200 million direct
from the NSF, and the rest from a variety of state sources. By 1995,
Usenet News was consuming about 8% of the bandwidth provided by the network.
If that&amp;rsquo;s a plausible overall measure of the relative cost of that traffic,
that means that Usenet News could have had as much as a $160 million subsidy
over 10 years to grow and expand on the public dime. A good system to
build on top of!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;One of the challenges of building a successful network is that at
some point you look like a system that can be exploited for other
people&amp;rsquo;s use, just in the same way that your system exploited some
aspect of the surrounding environment to get its toehold into the
world. Two moments in the early 1990s development of Usenet show
signs of what would be its demise as a discussion forum dominiated
by researchers and graduate students.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Up until September 1993, the ebb and flow of Usenet as a discussion
medium was pretty predictable. A network dominated by graduate students,
Usenet typically got a lot of new users in September, who floundered
around until they either dropped off the net or sorted out how to use
it effectively. A year later another September would roll around, and
last year&amp;rsquo;s new users were now this year&amp;rsquo;s seasoned veterans, showing
the newbies around or harrassing them or both.&lt;/p&gt;

&lt;p&gt;September 1993 changed all of that because America Online, the biggest
online service at the time, decided to add Usenet access to its millions
of commercial customers. Not surprisingly, these new users were different
from the ones that came before them - they were more often that not
far removed from the academic and research and hobbyist networks that
had come before them on Usenet. A culture clash ensued, and in some
ways Usenet never recovered.&lt;/p&gt;

&lt;p&gt;AOL&amp;rsquo;s embrace of Usenet is a classic &amp;ldquo;over the top&amp;rdquo; maneuver. AOL did
not have to hire any new moderation staff, run transcontinental networks, or
invent any new protocols to join in this existing network. Instead, they
could gather all of the benefits of providing their user base with access
to new services at a very small marginal cost.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Taking advantage of Usenet is also the theme of advertising networks
that took heed of Usenet&amp;rsquo;s worldwide low cost distribution system to
plant their own unsolicited commercial messages.&lt;/p&gt;

&lt;p&gt;Usenet had always been somewhat commercialized, ever since the first
person posted an ad for a dinette set for sale in New Jersey. Employment
advertising was tolerated or even welcomed, and regional &amp;ldquo;forsale&amp;rdquo;
groups prefigured Craigslist in many ways. But there was a social contract
that kept cooperating systems away from wholesale advertising. Sending
articles around the world costs money, and if the powers that be
knew that you were sending ads around on their dime they&amp;rsquo;d kick you
(or us) off the net.&lt;/p&gt;

&lt;p&gt;By 1994, though, the social contract had weakened a bit. Commericial
internet service providers were selling dialup Usenet accounts, and
AOL was offering rec.pets.cats to the masses. How natural would it be
for someone to observe that they could get huge numbers of ad impressions
for little or no money to a receptive audience?&lt;/p&gt;

&lt;p&gt;Enter the &amp;ldquo;Green Card Lawyers&amp;rdquo;, Canter and Siegel.
With the help of a programmer they designed an ad campaign that would
promote their immigrant legal aid to over 5500 separate newsgroups at
once. This burst of traffic - a separate ad for each and every Usenet
newsgroup they could find - was so substantial that they managed to
take some parts of the network offline (New Zealand, in particular,
which had some of the highest costs of importing Usenet anywhere, disconnected
temporarily rather than transmit the onslaught).&lt;/p&gt;

&lt;p&gt;Sadly, the Usenet community response to the Green Card Lawyers and
their ilk was a sign that the community infrastructure of Usenet
was going to have to adapt to a world where some members of the network
were hostile and were going to try to subvert the cooperative enterprise
with advertising. Unsolicited ads all sorts of things followed -
sketchy pharmaceutical advertisements promoting male enhancement drugs
were perhaps the most typical. The Usenet architecture started to crumble,
as people looking to escape the noise fled to private email lists,
and as institutions supporting the net looked twice at their costs and
decided it might not be worth it.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I keep coming back to Usenet because it&amp;rsquo;s so fascinating, and I spent
so much time there, and surprisingly enough it&amp;rsquo;s still going. What can
over the top networks learn from its surprising longevity, even though
several generations of technologists have given up on it?&lt;/p&gt;

&lt;p&gt;Usenet is the prototypical mesh network. Individual nodes in the network
connect to each other, without needing any central authority to bless them.
They connect using whatever technology they agree to, without needing
to adhere to any central standard except for the format of news articles.
The network works with a &amp;ldquo;flooding&amp;rdquo; protocol that distributes texts to
all parts of the network that are connected that want to receive them,
and aspects of the flooding algorithm ensure efficiency and that no
one gets an article sent to them twice. For a 1979 design, it&amp;rsquo;s held up well.&lt;/p&gt;

&lt;p&gt;What has changed irrevocably from the original is the content on the network.
Once upon a time, in the days before widespread use of the Internet, it
served as an amalgam of what emerged as all sorts of independent commercial
services. Modern day Github is an echo of the old comp.sources.* hierarchy,
delivering source code and patches and enhancements to your door. Quora
and Stack Overlow try to replicated the comp.* newsgroups, and
Reddit handles most of rec.*. Many if not most of the original newsgroups
are dead, overrun by spam.&lt;/p&gt;

&lt;p&gt;What lives on is the alt.* newsgroups, in particular alt.binaries.*. These
were never welcomed by the old guard who remembered how expensive it
was to transmit anything over a modem, but now over 30 years later
the cost of the network is so low.&lt;/p&gt;

&lt;p&gt;The modern Usenet ends up being a vast expanse of digitized video,
and a host of specialized newsreaders designed to quickly scoop
down the movie or TV show that you want to watch. The distribution
and flooding algorithms are perfect for this task, and copyright
owners have to work extra hard because even taking down one copyrighted
work from one part of the network doesn&amp;rsquo;t prevent you from pulling it
from some other area.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I want to fast forward a bit from the 1990s to the modern day, to
illustrate that all sorts of contemporary networks have bootstrapped
their way to success on top of other, older infrastructure.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The first common pattern I&amp;rsquo;ll call &amp;ldquo;identity proxy&amp;rdquo;. The idea here is
that you can take advantage of an existing organization&amp;rsquo;s identification
process, without ever having to know someone&amp;rsquo;s login and password.
By extending privileges to anyone who can prove that they are a
member of some other organization, you can scoop up valuable early
users.&lt;/p&gt;

&lt;p&gt;Facebook is one excellent example of this, especialy in its early days.
How do you restrict access to your brand new social network so that only
the social elites can participate? Simple, the onboarding process for
your new network runs over the top of the email system of the network
you want to attract from. Anyone who wants an account has to prove
that they can get an email from a harvard.edu address. Voila, you have
just run your login system over the top of a very expensive and valuable
college admissions process, at minimal cost to yourself.&lt;/p&gt;

&lt;p&gt;Slack runs the same strategy when trying to recruit companies to use its
product. You can configure a Slack instance so that anyone who has an
email address in a particular domain is automatically approved. This gives
Slack leverage to gather employees for its &amp;ldquo;free&amp;rdquo; service and insight
to get those people into its paid service.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Identity proxy&amp;rdquo; is a powerful pattern. If you find the right trove
of people who have a desirable email address, you can scoop them up
into your system and exclude others who don&amp;rsquo;t have the right qualifying
talisman.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The state of the city, Ann Arbor, fall 2015</title>
      <link>http://vielmetti.github.io/post/2015/2015-10-14-the-state-of-the-city/</link>
      <pubDate>Wed, 14 Oct 2015 10:30:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-10-14-the-state-of-the-city/</guid>
      <description>&lt;p&gt;A perspective on Ann Arbor area local government, as seen from fall 2015.&lt;/p&gt;

&lt;p&gt;The top level view is that local units of government are struggling
with their technology investments, having a difficult time keeping
them working, and are unwilling or unable to commit to any sort of
&amp;ldquo;open data&amp;rdquo; initiatives as a result of their difficulties with basic
operations.&lt;/p&gt;

&lt;p&gt;Some examples will illustrate.&lt;/p&gt;

&lt;p&gt;The City of Ann Arbor keeps track of a wide range of planning and
permit documents through a vendor system called eTrakit. This system
was down for more than a week in September, and still occasionally
crashes in response to a query. There is no general purpose &amp;ldquo;open data&amp;rdquo;
query to get access to the system, though the city does publish a
&amp;ldquo;permits under review&amp;rdquo; feed on its open data catalog that has a small
subset of the available data. Attachments in the system were recently
taken offline after unredacted credit card numbers were discovered.&lt;/p&gt;

&lt;p&gt;City of Ann Arbor meeting agendas and minutes and documents are locked up
in a vendor-provided Legistar system, which is so difficult to use that
many boards and commissions don&amp;rsquo;t even bother to publish their minutes
through that system. Meeting agendas are often delayed in publication,
and there is no method available to search across the whole system for
documents.&lt;/p&gt;

&lt;p&gt;The Ann Arbor City Clerk keeps track of FOIA requests across the city
and publishes a &amp;ldquo;database&amp;rdquo; of requests. As is all too typical for
municipal government, this &amp;ldquo;database&amp;rdquo; is published as a PDF file, not
as plain text in some format that can be easily parsed.&lt;/p&gt;

&lt;p&gt;The City Liquor License Review Committee routinely evaluates applications
for new permits to serve or manufacture alcohol. These applications
are not published as part of the agenda for that committee or for
Ann Arbor City Council&amp;rsquo;s online system.&lt;/p&gt;

&lt;p&gt;I would go on, but the City Council meeting is tonight, and I&amp;rsquo;m
preparing something for public commentary (probably to add to the
discussion only after the decisions have been made).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>August 2015 food inspections for Washtenaw County</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-22-august-2015-food-inspections/</link>
      <pubDate>Tue, 22 Sep 2015 08:30:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-22-august-2015-food-inspections/</guid>
      <description>&lt;p&gt;Every month the Washtenaw County Health Department releases reports from a previous month&amp;rsquo;s
inspections of restaurants in the county. Restaurants are typically inspected twice a year.
The inspector looks to see that the state Food Code is enforced, with a particular eye
towards identifying violations that result in increased risk to the public of food-borne illness.&lt;/p&gt;

&lt;p&gt;Wayne Eaker has put together a searchable and browsable database of these inspections,
which is online at &lt;a href=&#34;http://food-inspections.annarbortelegraph.com&#34;&gt;food-inspections.annarbortelegraph.com&lt;/a&gt; . Here are some highlights
and lowlights from the August 2015 report.&lt;/p&gt;

&lt;p&gt;19 restaurants and food service establishments had &lt;a href=&#34;http://food-inspections.annarbortelegraph.com/?f[0]=field_inspection_date%3A2015&amp;amp;f[1]=field_inspection_date%3A2015-08&amp;amp;f[2]=field_count_total%3A%5B0%20TO%200%5D&#34;&gt;zero violations in August 2015&lt;/a&gt;. Among those perfect scores:
Zingerman&amp;rsquo;s Next Door, Earthen Jar, BTB Cantina on South U,
Little Porky&amp;rsquo;s in Whitmore Lake, World of Beers,
Classic Pizza in Dexter, and Sweetwaters on Plymouth Road.&lt;/p&gt;

&lt;p&gt;Yee Siang Dumplings at 4837 Washtenaw received
a total of 17 violations.
At the &lt;a href=&#34;http://food-inspections.annarbortelegraph.com/2015-08-21-yee-siang-dumplings&#34;&gt;August 21, 2015 inspection&lt;/a&gt;
the issues identifed included
&amp;ldquo;unsafe cooling methods&amp;rdquo;,
a dirty meat slicer and meat grinder,
and no soap at the rear employee hand sink.
&amp;ldquo;NO ONE IN THIS STORE HAS ANY KNOWLEDGE OF FOOD SAFETY AND HOW TO IMPLEMENT THE FOOD CODE IN THEIR OPERATION.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Tony Sacco&amp;rsquo;s on W Eisenhower Pkwy
had 19 violations
in their &lt;a href=&#34;http://food-inspections.annarbortelegraph.com/2015-08-05-tony-saccos&#34;&gt;August 5, 2015 inspection&lt;/a&gt;.
&amp;ldquo;OBSERVED LIVE ANTS AND NUMEROUS SMALL FLIES IN THE AREAS SURROUNDING THE FLOOR DRAIN IN THE WAITSTATION.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Grange Kitchen and Bar was dinged for
&amp;ldquo;OBSERVED UNLABELED MASON JARS OF DUCK BLOOD IN CHEST FREEZER IN KITCHEN.&amp;rdquo;
Their &lt;a href=&#34;http://food-inspections.annarbortelegraph.com/2015-08-27-grange-kitchen-bar&#34;&gt;August 27, 2015 inspection&lt;/a&gt;
recorded a total of 8 violations.&lt;/p&gt;

&lt;p&gt;Aventura at 216 East Washington received a total of 7 violations
in their &lt;a href=&#34;http://food-inspections.annarbortelegraph.com/2015-08-18-aventura&#34;&gt;August 18, 2015 inspection&lt;/a&gt;.
&amp;ldquo;OBSERVED SEVERAL EMPLOYEES IN BAR AREA HANDLE RTE FOOD WITH BARE HANDS&amp;rdquo; was noted, as well as
a practice of re-labeling datemarked foods to extend their shelf life.
&amp;ldquo;EMPLOYEE STATED THAT IT IS COMMON PRACTICE FOR KITCHEN STAFF TO SMELL OR TASTE FOOD PRODUCTS WHEN THEY ARE ON THEIR LAST DAY OF SALE AND RE-DATE FOOD FOR ANOTHER 5 DAYS IF THEY THINK THE FOOD IS STILL GOOD. THIS PRACTICE MUST BE CEASED IMMEDIATELY.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Check the whole list and see how your favorites did.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to squash a commit in git</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-22-git-squash/</link>
      <pubDate>Tue, 22 Sep 2015 07:15:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-22-git-squash/</guid>
      <description>

&lt;p&gt;If you are working on a project in Github, your work isn&amp;rsquo;t always linear.
Sometimes the change you make doesn&amp;rsquo;t work completely, so you back a little
bit of it out and move forward on a next part. It may take some hacking and
sawing (and testing of course) to get a change that just works right.&lt;/p&gt;

&lt;p&gt;Projects like to have contributions, but project maintainers don&amp;rsquo;t like
to insert noise into their efforts. The ideal commit to a project is
completely clean: one single commit that incorporates all of the changes
that you want to do, without any of the intervening cruft.&lt;/p&gt;

&lt;p&gt;Thanks to Alan Gutierrez for the help here.&lt;/p&gt;

&lt;p&gt;In the lingo, this is known as &amp;ldquo;squashing a commit&amp;rdquo;. It turns
&lt;code&gt;A-&amp;gt;B-&amp;gt;C-&amp;gt;D-&amp;gt;X-&amp;gt;Y-&amp;gt;Z&lt;/code&gt; into &lt;code&gt;A-&amp;gt;Z&lt;/code&gt;. Here&amp;rsquo;s how you do it.&lt;/p&gt;

&lt;p&gt;First, set up your environment before you start to work. Create a branch
to put your changes in and work within that branch. You can do all of that
from within Github, or from the command line as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% git checkout -b try/branch
Switched to a new branch &#39;try/branch&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you&amp;rsquo;re ready to merge, create a pull request for that branch.
That will gather together all of the changes you have made and drop
them onto a single page for inspection. Look at the first change that
you made in that sequence, and find the parent node for your first
change; we&amp;rsquo;ll rebase off of that parent. This was the crucial bit
that I was unable to figure out from the documentation.&lt;/p&gt;

&lt;p&gt;Squash all of those commits for the branch
into a single commit. There is no &lt;code&gt;git squash&lt;/code&gt; command, alas; but there
is &lt;code&gt;git rebase -i&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% git rebase -i &amp;lt;after-this-commit&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, apply this single squashed commit to your master branch.&lt;/p&gt;

&lt;h3 id=&#34;further-reading:d402ad0362f9cb748dc3191a35343d42&#34;&gt;Further reading:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;From Git Ready: &lt;a href=&#34;http://gitready.com/advanced/2009/02/10/squashing-commits-with-rebase.html&#34;&gt;Squashing commits with rebase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Makandra Cards: &lt;a href=&#34;http://makandracards.com/makandra/527-squash-several-git-commits-into-a-single-commit&#34;&gt;Squash several Git commits into a single commit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;David Walsh: &lt;a href=&#34;http://davidwalsh.name/squash-commits-git&#34;&gt;Squash commits with Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pro Git,  Scott Chacon and Ben Straub: &lt;a href=&#34;http://www.git-scm.com/book/en/v2/Git-Tools-Rewriting-History&#34;&gt;Git Tools - Rewriting History&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Snap CI</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-22-snap-ci/</link>
      <pubDate>Tue, 22 Sep 2015 07:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-22-snap-ci/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://snap-ci.com&#34;&gt;Snap CI&lt;/a&gt; is a cloud-based continuous integration system. Like
Travis CI, which I have written about &lt;a href=&#34;http://vielmetti.github.io/post/2015/2015-09-16-travis-ci/&#34;&gt;here&lt;/a&gt;,
it lives in the cloud, lets you run programs triggered by GitHub commits, and
offers a testing environment for free for open source projects which lets you
familiarize yourself with the system before deciding to pay for it for your
non-open-source efforts.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m looking at all of these tools now because this weblog is now managed through
a process that involves checking things into Github, which makes it completely
suitable for automation for common tasks every time I save a file. (Spell checking,
link checking, really any kind of quality control you can imagine reducing to
a tiny shell script.)&lt;/p&gt;

&lt;p&gt;Like Travis CI, Snap CI lets you automate the build process. Where it diverges
(and where it&amp;rsquo;s interesting) is that the build process is only the first part of
a multi-stage pipeline that includes deployment at the end of the task only when
a set of human decisions have been made as to whether the thing you made is ready
to be shared.&lt;/p&gt;

&lt;p&gt;This &lt;a href=&#34;https://blog.snap-ci.com/blog/2014/07/22/why-snapci-and-travisci-are-not-same-thing/&#34;&gt;Snap CI blog post&lt;/a&gt;
on why Travis CI and Snap CI are not the same thing is from Snap CI and it&amp;rsquo;s a useful
overview.&lt;/p&gt;

&lt;p&gt;The downside of multi-stage pipelines for deployment is that they are very hard to
automate correctly, and very hard to demonstrate correct behavior with open source
projects that generally struggle to have any automated test facilities at all. So
while Travis CI is a very good fit for open source components (and has attracted a
whole sub-industry of writing &lt;code&gt;.travis.yml&lt;/code&gt; files to support those), it really doesn&amp;rsquo;t
have the whole &amp;ldquo;continuous deployment&amp;rdquo; pattern sorted out. And while Snap CI looks
like it would be great for saying &amp;ldquo;once we&amp;rsquo;re happy with this, push it out to the world&amp;rdquo;,
that setup is not easy to pick up simply by inspecting other projects that have done it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AWS DynamoDB downtime, Sunday am, September 20, 2015</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-20-aws-dynamodb-downtime-sunday-am/</link>
      <pubDate>Sun, 20 Sep 2015 09:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-20-aws-dynamodb-downtime-sunday-am/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;A distributed system is one in which the failure of a computer
you didn&amp;rsquo;t even know existed can render your own computer unusable.
&lt;a href=&#34;http://research.microsoft.com/en-us/um/people/lamport/pubs/distributed-system.txt&#34;&gt;Leslie Lamport, 1987&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Amazon Web Services DynamoDB experienced downtime in the N Virginia
availability zone early Sunday morning, September 20, 2015. As
a result, a number of other AWS services inside N Virginia that
depend on DynamoDB also had downtime. Companies and organizations
that built services on top of those systems who didn&amp;rsquo;t have
geographic load balancing were having problems as well.&lt;/p&gt;

&lt;p&gt;UPDATE: Amazon&amp;rsquo;s report &lt;a href=&#34;https://aws.amazon.com/message/5467D2/&#34;&gt;Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region&lt;/a&gt; is available as of September 23, 2015.&lt;/p&gt;

&lt;p&gt;Affected services include at least CloudWatch, SES, SNS, SQS, SWS,
AutoScale, Cloud Formation, Directory Service, Key Mgmt and Lambda,
according to a report on Hacker News.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://status.aws.amazon.com/&#34;&gt;http://status.aws.amazon.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Reports from &lt;a href=&#34;https://www.twitter.com/ylastic&#34;&gt;@ylastic&lt;/a&gt; on Twitter
have been helpful in keeping track of outages. &lt;a href=&#34;http://www.ylastic.com&#34;&gt;http://www.ylastic.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Down Detector has a page of AWS outages trouble reports, and
details pulled from Twitter. &lt;a href=&#34;https://downdetector.com/status/aws-amazon-web-services&#34;&gt;https://downdetector.com/status/aws-amazon-web-services&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When core infrastructure goes down, it tends to affect other platforms
that depend on that core infrastructure and that hide it from their
users. This in turn affects applications built on those platforms.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Docker: &amp;ldquo;We are currently seeing intermittent errors when pushing
and pulling, related to issues that AWS is having. We are currently
investigating the causes, and doing what we can to mitigate the
problems.&amp;rdquo; @dockerstatus and &lt;a href=&#34;http://status.docker.com/&#34;&gt;http://status.docker.com/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Heroku: &amp;ldquo;Starting new dynos (unidling, one-off, scaling or
restarting crashed apps, new releases) is still unavailable.&amp;rdquo; &amp;ldquo;Until
this incident is resolved, you might be unable to open new support
tickets with us. If you need to communicate with our support staff
during this time, please email outage-support@heroku.com.&amp;rdquo; @herokustatus
and &lt;a href=&#34;https://status.heroku.com/incidents/811&#34;&gt;https://status.heroku.com/incidents/811&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CircleCI: &amp;ldquo;Experiencing Issues with Heroku and AWS&amp;rdquo;. @circleci
and &lt;a href=&#34;http://status.circleci.com/&#34;&gt;http://status.circleci.com/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TravisCI: &amp;ldquo;Partial System Outage&amp;rdquo;. @traviscistatus and
&lt;a href=&#34;https://www.traviscistatus.com/&#34;&gt;https://www.traviscistatus.com/&lt;/a&gt; After incident report
at &lt;a href=&#34;https://www.traviscistatus.com/incidents/wzyhx97450f4&#34;&gt;https://www.traviscistatus.com/incidents/wzyhx97450f4&lt;/a&gt; ,
&amp;ldquo;Degradations and outages due to AWS us-east-1 domino effect&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are a lot of applications built on AWS and on Heroku,
which are at risk of downtime. A comprehensive list is probably
impossible, but here are some reports, in alphabetical order.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Airbnb&lt;/li&gt;
&lt;li&gt;Amazon Echo&lt;/li&gt;
&lt;li&gt;Amazon Instant Video&lt;/li&gt;
&lt;li&gt;AWS Console. The API is hosted in each region, so the command line worked, but the console is hosted out of us-east-1 and was unavailable.&lt;/li&gt;
&lt;li&gt;AWS Support Tickets. Use this &lt;a href=&#34;http://www.amazon.com/gp/html-forms-controller/support-center-issues-u&#34;&gt;workaround&lt;/a&gt; ???&lt;/li&gt;
&lt;li&gt;Buffer&lt;/li&gt;
&lt;li&gt;Canopy. &amp;ldquo;Our iOS app is down due to the system-wide #AWSoutage. Updates will be provided soon.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Canva&lt;/li&gt;
&lt;li&gt;Clickfunnels. &amp;ldquo;Our development team is aware of this, however, at this time, the only thing that we can do is wait until the issues at Amazon AWS are resolved completely.&amp;rdquo; &lt;a href=&#34;http://status.clickfunnels.com/&#34;&gt;http://status.clickfunnels.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CoinSimple. &amp;ldquo;Widespread outage in Amazon Web Services is affecting several Internet services, including @CoinSimple. Please check here for updates.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;DuoLingo&lt;/li&gt;
&lt;li&gt;FastSpring. &amp;ldquo;We were having some intermittent problems due to #awsoutage. The problem is being resolved by Amazon.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;GroupMe&lt;/li&gt;
&lt;li&gt;HashiCorp. &amp;ldquo;Our infrastructure provider is experiencing issues which may cause our project websites to be inaccessible. Sorry for the inconvenience.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;IFTTT. &amp;ldquo;We have identified an issue with our service provider. We will continue to provide updates as more information becomes available.&amp;rdquo; &lt;a href=&#34;http://status.ifttt.com/incidents/xnbwnqj608hg&#34;&gt;http://status.ifttt.com/incidents/xnbwnqj608hg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IMDB&lt;/li&gt;
&lt;li&gt;Lightspeed POS. &amp;ldquo;Monitoring - Our service provider is experiencing a major issue with several of their services. Our engineers are working to reduce our reliance on these components until their issues are resolved. Some customers may experience slowness related to this problem.&amp;rdquo; &lt;a href=&#34;http://status.lightspeedretail.com/&#34;&gt;http://status.lightspeedretail.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mediacom&lt;/li&gt;
&lt;li&gt;Medium. &amp;ldquo;Identified - We&amp;rsquo;re working to fix a major outage and will be back online as soon as possible.&amp;rdquo; &lt;a href=&#34;https://medium.statuspage.io/&#34;&gt;https://medium.statuspage.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Nest. &amp;ldquo;Weâ€™re investigating a service outage with the Nest mobile app and Cam services, and the team is working on a fix. Details to come.&amp;rdquo; &lt;a href=&#34;https://nest.com/support/#status&#34;&gt;https://nest.com/support/#status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Netflix. &amp;ldquo;We are currently experiencing issues streaming on all devices.
We are working to resolve the problem. We apologize for any inconvenience.&amp;rdquo; &lt;a href=&#34;https://help.netflix.com/help&#34;&gt;https://help.netflix.com/help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paper. &amp;ldquo;account related activities in Paper are down due to problems with our infrastructure provider #AWS&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Pocket.&lt;/li&gt;
&lt;li&gt;Product Hunt. &amp;ldquo;Our servers are sick. Weâ€™re working on it right meow ðŸ˜»&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Reddit.&lt;/li&gt;
&lt;li&gt;Quandl. &amp;ldquo;The Quandl website and API are temporarily unavailable, due to problems at #AWS.&amp;rdquo;  &lt;a href=&#34;http://quandl.statuspage.io&#34;&gt;http://quandl.statuspage.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Shipt. &amp;ldquo;Unfortunately, we&amp;rsquo;re down right now with the #AWSOutage.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Social Flow&lt;/li&gt;
&lt;li&gt;Takipi. &amp;ldquo;Due to an AWS outage, we&amp;rsquo;re experiencing some slowness and connectivity issues. Stay tuned for updates&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Tinder.&lt;/li&gt;
&lt;li&gt;Twilio. &amp;ldquo;Update - We have established the DynamoDB backend in another AWS region and are re-routing requests from upstream services to the new region. We will provide another update once we verify that the requests are being serviced properly. &amp;rdquo; &lt;a href=&#34;http://status.twilio.com/&#34;&gt;http://status.twilio.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Viber. &amp;ldquo;We are experiencing disruptions in our service. We are working to resolve this issue. Sorry for the inconvenience.&amp;rdquo; &lt;a href=&#34;https://support.viber.com/&#34;&gt;https://support.viber.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;waffle.io &amp;ldquo;Experiencing Issues with Heroku&amp;rdquo; &lt;a href=&#34;http://status.waffle.io/&#34;&gt;http://status.waffle.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Walt Disney World app.&lt;/li&gt;
&lt;li&gt;Wink.  &amp;ldquo;We are noticing increased connection issues for Wink users. Our engineers are working with Amazon to get this resolved ASAP.&amp;rdquo; &lt;a href=&#34;http://status.winkapp.com/&#34;&gt;http://status.winkapp.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Follow the discussion on Twitter on the #AWS hashtag, as well as
#awsoutage and #awsdown . Twitter is unaffected by this outage,
and Slack is also unaffected, which devops teams are both happy about.&lt;/p&gt;

&lt;h3 id=&#34;news-coverage:0bf1024f29774b00d48e1402990b91e9&#34;&gt;News coverage&lt;/h3&gt;

&lt;p&gt;This &lt;a href=&#34;https://news.ycombinator.com/item?id=10247307&#34;&gt;Hacker News discussion&lt;/a&gt;
is a good one as startups radio in their woes.&lt;/p&gt;

&lt;p&gt;TNW: &lt;a href=&#34;http://thenextweb.com/insider/2015/09/20/amazon-web-services-goes-down-taking-netflix-reddit-pocket-and-more-with-it/&#34;&gt;Amazon Web Services goes down, taking Netflix, Reddit, Pocket and more with it&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Register: &lt;a href=&#34;http://www.theregister.co.uk/2015/09/20/aws_database_outage/&#34;&gt;AWS knocks Amazon, Netflix, Tinder and IMDb offline in MEGA data collapse; Cloudopocalypse stalks Sunday sofa surfers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;VentureBeat: &lt;a href=&#34;http://venturebeat.com/2015/09/20/amazons-aws-outage-takes-down-netflix-reddit-medium-and-more/&#34;&gt;Amazonâ€™s AWS outage takes down Netflix, Reddit, Medium, and more
&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Into the matrix with Travis CI</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-16-travis-ci/</link>
      <pubDate>Wed, 16 Sep 2015 09:22:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-16-travis-ci/</guid>
      <description>

&lt;p&gt;Travis CI is a continuous integration tool, a description which means nothing
to people who have never used it. &amp;ldquo;Continuous integration&amp;rdquo; is one of these
industry phrases that doesn&amp;rsquo;t serve to illuminate the problem that it describes.&lt;/p&gt;

&lt;p&gt;Why would you care? Well, you&amp;rsquo;re working on a project with a bunch of other
people, and it&amp;rsquo;s kind of complicated. It might depend on some
&lt;a href=&#34;http://vielmetti.github.io/post/2015/2015-09-14-old-code-and-old-tools/&#34;&gt;old code&lt;/a&gt;
that&amp;rsquo;s fragile and should be tested all the time when it changes, or it might depend
some brand-spanking new code that doesn&amp;rsquo;t quite work and you know it
doesn&amp;rsquo;t work yet but you want the test system to be less tedious.&lt;/p&gt;

&lt;p&gt;What else looks like this?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://circleci.com/&#34;&gt;CircleCI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://snap-ci.com/&#34;&gt;Snap CI&lt;/a&gt; by ThoughtWorks&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.solanolabs.com/&#34;&gt;Solano&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.appveyor.com/&#34;&gt;Appveyor&lt;/a&gt; (for Windows)&lt;/li&gt;
&lt;li&gt;Jenkins (self hosted)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What does this look like from a workflow perspective? Every time you check in a
file, the whole system builds, and all of the tests you have configured run.
With proper configuration you can build a matrix of versions or configurations
to test, so instead of a test running on a single system it can run on
3x3 or 2x4 or even 3x5x2 configurations.&lt;/p&gt;

&lt;h3 id=&#34;ruby:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Ruby&lt;/h3&gt;

&lt;p&gt;Travis CI&amp;rsquo;s default langauge is Ruby, and a lot of its tooling is written
in Ruby, so its Ruby support is quite good.&lt;/p&gt;

&lt;p&gt;Need a bunch of versions of Ruby? Use &lt;code&gt;rvm&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.travis-ci.com/user/languages/ruby/&#34;&gt;Building a Ruby Project, Travis CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;c-and-c:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;C and C++&lt;/h3&gt;

&lt;p&gt;The version of gcc and g++ on the default systems is 4.6, and you can
pull in 4.8 through &amp;lsquo;apt&amp;rsquo;, but you currently have to pick up 4.8 through
the &lt;code&gt;ubuntu-toolchain-r-test&lt;/code&gt; source and not simply by picking a gcc version.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rla/fast-feed/blob/master/.travis.yml&#34;&gt;rla/fast-feed/.travis.yml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;go-golang:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Go (golang)&lt;/h3&gt;

&lt;p&gt;Go (golang) has not gone through a lot of versions yet, and its latest 1.5
build is written in Go. Because the language is so young, there is not lots
of old code to port, and thus not too many version dependencies in libraries
to worry about yet.&lt;/p&gt;

&lt;p&gt;Testing two versions of Go (golang) plus Linux and Mac is easy, and the
configuration file doesn&amp;rsquo;t have to do anything funky.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/miekg/mmark/fast-feed/blob/master/.travis.yml&#34;&gt;miekg/mmark/.travis.yml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;nodejs:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;NodeJS&lt;/h3&gt;

&lt;p&gt;Node 4.0 is new, and the source currently of a fair bit of porting efforts as
a lot of things have broken not in the language but in its libraries. Fortunately,
the nodejs and iojs split has been reconciled, but unfortunately there&amp;rsquo;s about
six months of reunification work to be done.&lt;/p&gt;

&lt;p&gt;If you are porting or building to NodeJS 4.0, you have to drag in gcc 4.8. Previous
versions of gcc/g++ aren&amp;rsquo;t capable of coping with the C++11 constructs that are
in V8 and (especially) in &lt;code&gt;nan&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;NodeJS 4.0 on Mac? In addition to gcc 4.8, you have to drag in nvm.
The approachable way is to pull in the source from Github that lives
at &lt;code&gt;https://github.com/creationix/nvm.git&lt;/code&gt;, build it, and then use the
thing that you just built to subsequently pull in the right version of Node.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rla/fast-feed/blob/master/.travis.yml&#34;&gt;rla/fast-feed/.travis.yml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sass/node-sass/blob/master/.travis.yml&#34;&gt;sass/node-sass/.travis.yml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;python:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Python&lt;/h3&gt;

&lt;p&gt;Python 2 and Python 3 are both available on Travis CI, including even
the latest Python 3.5.&lt;/p&gt;

&lt;p&gt;If you need three versions of Python plus three versions of Django:
use &amp;lsquo;env&amp;rsquo; to drive &amp;lsquo;pip&amp;rsquo;, and &amp;lsquo;matrix&amp;rsquo; to skip some builds where you
know that the combinations don&amp;rsquo;t work.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(datadesk/django-softhyphen)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lots and lots of Python versions, with different dependencies for
each version? The &lt;code&gt;tornado&lt;/code&gt; package has that.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(tornadoweb/tornado)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Looking to test Django plus Postgres?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;http://blog.schwuk.com/2014/06/13/using-travis-ci-for-testing-django-projects/&#34;&gt;http://blog.schwuk.com/2014/06/13/using-travis-ci-for-testing-django-projects/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;java:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Java&lt;/h3&gt;

&lt;p&gt;Java is supported.&lt;/p&gt;

&lt;p&gt;If you need Java 8 in a particular version, see the below issue on Github Issues.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/travis-ci/travis-ci/issues/4042&#34;&gt;Java 8 version update and bug&lt;/a&gt;,
Github issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;fortran:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Fortran&lt;/h3&gt;

&lt;p&gt;If you&amp;rsquo;re building scientific codes, Fortran is available through the &lt;code&gt;apt&lt;/code&gt;
mechanism.&lt;/p&gt;

&lt;h3 id=&#34;prolog:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Prolog&lt;/h3&gt;

&lt;p&gt;Prolog? Bring it in with apt-add-repository and apt-get. This is illustrative
of Travis CI&amp;rsquo;s ability to incorporate nearly any language, since you can install
new compilers and interpreters via package systems and then run them.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(rla/simple-template)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;multiple-langauges:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Multiple langauges&lt;/h3&gt;

&lt;p&gt;Two versions of Go plus Python plus an external dependency that&amp;rsquo;s being flaky?
Use &lt;code&gt;language: python&lt;/code&gt; because it&amp;rsquo;s easier to install &lt;code&gt;go&lt;/code&gt; from package
managers than it is to drag in the whole complex python multi-version infrastructure
from scratch.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(miekg/mmark, in progress)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;os-x:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;OS X&lt;/h3&gt;

&lt;p&gt;Use &amp;lsquo;osx_image&amp;rsquo; to tweak the version of &amp;lsquo;osx&amp;rsquo; that you get.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;http://docs.travis-ci.com/user/osx-ci-environment/&#34;&gt;http://docs.travis-ci.com/user/osx-ci-environment/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;(&lt;a href=&#34;http://blog.travis-ci.com/2015-09-09-xcode7-gm/&#34;&gt;http://blog.travis-ci.com/2015-09-09-xcode7-gm/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;both-linux-and-mac-in-general:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Both Linux and Mac, in general&lt;/h3&gt;

&lt;p&gt;Go is easy. (miekg/mmark)&lt;/p&gt;

&lt;p&gt;Node requires bringing in nvm and gcc 4.8. (rla/fastfeed)&lt;/p&gt;

&lt;p&gt;Python &amp;hellip; (don&amp;rsquo;t know)&lt;/p&gt;

&lt;p&gt;Prolog &amp;hellip; (don&amp;rsquo;t know)&lt;/p&gt;

&lt;p&gt;Ruby &amp;hellip; (should work pretty well, but not tested)&lt;/p&gt;

&lt;h2 id=&#34;acknowledgements:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thanks to the following people for their help: Raivo Laanemets (@RaivoL),
Mohan Kartha (@mckartha), Chris Dzombak (@cdzombak), &amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>todo</title>
      <link>http://vielmetti.github.io/sidebar/todo/</link>
      <pubDate>Tue, 15 Sep 2015 10:11:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/sidebar/todo/</guid>
      <description>

&lt;h2 id=&#34;todo:5b51c9e8380c18fc2ee8d866662d01d8&#34;&gt;Todo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Test integration (and maybe publishing too) with &lt;code&gt;travis&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Acceptance tests with &lt;code&gt;bats&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Auto-discovery for RSS feed, per &lt;a href=&#34;http://gohugo.io/templates/rss/&#34;&gt;http://gohugo.io/templates/rss/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Short codes for easy linking. First pass complete.
Assisted synaesthesia and colorized names and phrases.
(Started, with colorized but not linked names)&lt;/li&gt;
&lt;li&gt;Nicely thought through category and taxonomy pages. They
have stubs now, but they need design.
and it should integrate with Arborwiki. (Started.)&lt;/li&gt;
&lt;li&gt;Links from books to their corresponding URLs and ISBNs to
link to AADL and Amazon pages, perhaps with Librarything data&lt;/li&gt;
&lt;li&gt;Pagination with group by date, coming in 0.15&lt;/li&gt;
&lt;li&gt;Fix problem with dots in taxonomy names, coming in 0.15&lt;/li&gt;
&lt;li&gt;Some kind of database driven page data, in preparation
for rolling out a power outage maps collection based on DOE EIA data,
cf &lt;a href=&#34;http://gohugo.io/extras/datafiles/&#34;&gt;http://gohugo.io/extras/datafiles/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Make sure that the theme is in version control, which means
I have to learn about git subtrees.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Zeno&#39;s Inbox</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-14-zenos-inbox/</link>
      <pubDate>Mon, 14 Sep 2015 01:15:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-14-zenos-inbox/</guid>
      <description>&lt;p&gt;Zeno&amp;rsquo;s inbox: cut in half in an hour; it always takes an hour to
cut it in half, no matter how few messages there are in it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Old code and old tools</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-14-old-code-and-old-tools/</link>
      <pubDate>Mon, 14 Sep 2015 00:05:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-14-old-code-and-old-tools/</guid>
      <description>&lt;p&gt;One of the problems with old codebases is that they are missing
modern tooling for automated builds, automated testing, and the
like.&lt;/p&gt;

&lt;p&gt;A second problem with old code is that the maintainers (absent
modern tooling) may not know or recognize what dependencies they
have.&lt;/p&gt;

&lt;p&gt;If an old package is not mostly self-contained, it&amp;rsquo;s likely to
have dependencies on other old code. This makes it hard to build,
as those older dependencies have either gone offline or have mutated
so that the old package doesn&amp;rsquo;t work with the new releases.&lt;/p&gt;

&lt;p&gt;To confound the matter even more, the newer tools for automated
builds, automated tests and so forth have their own sets of complex
dependencies, so much so that unless you are around some team that
has already absorbed these systems that you are unlikely to grasp
all of their deeply interconnected complexities.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s easy to cascade into a deep chasm of interlocking dependencies,
yielding enough complexity in the build process to frustrated people
who really care more about the code than how it&amp;rsquo;s packaged for distribution.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;I&amp;rsquo;ve been working this weekend on &lt;code&gt;tidy-html5&lt;/code&gt;. It has a Makefile
that&amp;rsquo;s generated by CMake, and its documentation is built with
a combination of xsltproc and Doxygen. As dependencies go that&amp;rsquo;s
not too bad, but the problem starts because the codebase lay dormant
for a couple of years, and somewhere between three and six years
of net development and build tooling efforts have passed it by.&lt;/p&gt;

&lt;p&gt;There was no Dockerfile for the project, so I wrote one to see if I
could get it to build and could understand all of the build dependencies.
That took a dozen tries, but eventually something went through to
completion.&lt;/p&gt;

&lt;p&gt;The first thing I noted was that it not only depended on CMake but
also a particularly relatively recent version of CMake. Ditto for
Doxygen - not any elderly Doxygen will do, but only a relatively
recent build. Doxygen has its own dependencies, lots of them - about
a gigabyte of code to download and install.&lt;/p&gt;

&lt;p&gt;Next was to pick up on a previous effort to build the whole thing under
Travis CI. Now I have a relatively old build system that doesn&amp;rsquo;t
have these old dependencies so I have to drag them in, and not only
drag in some version but a particularly new version.&lt;/p&gt;

&lt;p&gt;Dependencies satisfied, it&amp;rsquo;s time to build. I want this to work on my
Raspberry Pi, so I build it there, and discover that the version of
Doxygen there is too old (oops) so it doesn&amp;rsquo;t build the docs at all,
never mind the cheery message from the build script that claims
&amp;ldquo;TidyLib API documentation has been built&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Next, to test. Does it pass all of its tests? Well, there is a test
directory with a bunch of test cases, but the documentation says things
like&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Of course some of the tests were to say avoid a segfault found.
Other tests were to visually compare the original input test file
in a browser, with how the new output displayed in a browser. This
is a purely VISUAL compare, and can not be done in code.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Frankly, a test that can&amp;rsquo;t be done in code is one that I can&amp;rsquo;t run
in finite time. I build all of the tests, diff them with the reference
directory, and hope that the differences (there are a few) aren&amp;rsquo;t fatal.&lt;/p&gt;

&lt;p&gt;You see where this is going&amp;hellip;&lt;/p&gt;

&lt;p&gt;Frankly, it&amp;rsquo;s hard to get all of the tooling right. Travis CI has all of
its own quirks, and you have to set up a build correctly to get correct
build completion results. Once the build happens, you want some sort of
test harness to take all of these test cases and turn them into red lights
and green lights on your screen, and I don&amp;rsquo;t have that yet. Building
binaries is yet another task - one project contributor put together a
Jenkins install to take on that task.  And so forth.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;The piece of this that I care about is twofold. &lt;code&gt;tidy&lt;/code&gt; as it stands now
is frozen in 2009, and that&amp;rsquo;s before HTML5. &lt;code&gt;tidy-html5&lt;/code&gt; is caught in flux,
with a successful project rescue to get it to build at all, but not yet
the full-on automation and test infrastructure that projects like &lt;code&gt;docker&lt;/code&gt;
or &lt;code&gt;node.js&lt;/code&gt; have that allow for ferocious parallel development with
reasonable test and integration coverage.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;tidy&lt;/code&gt; project is very old (20 yrs), and if it&amp;rsquo;s going to pass
a full suite of automated tests on every check-in for every platform,
that&amp;rsquo;s going to take some consolidated effort - both by people who
care about the code (but not so much how it&amp;rsquo;s packaged) and by people
who care about the packaging (so that they can use the code). Not
only does the code need to evolve, but the build tools around the
code need to move forward.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing up your tweets</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-14-writing-up-your-tweets/</link>
      <pubDate>Mon, 14 Sep 2015 00:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-14-writing-up-your-tweets/</guid>
      <description>&lt;p&gt;I use Twitter regularly, and will frequently post short things there
that reflect some 140 character slice of what I&amp;rsquo;m thinking about. That&amp;rsquo;s
very quick, but at some level deeply unsatisfying, because how much can
you put into 140 characters? Never quite enough.&lt;/p&gt;

&lt;p&gt;The challenge is to write up your tweets into something longer. Take
the quip and turn it into a lead paragraph, then expand the lead paragraph
into three or five or fifteen or fifty more. Explore at leisure, including
excursions into thinking that don&amp;rsquo;t lend themselves to quippiness.&lt;/p&gt;

&lt;p&gt;In retrospect some tweets would have better been unwritten. The process
of twittering out a stream of observations gets in the way of assembling
the longer context. We focus on the stream of things coming our way,
never taking time to take stock of the situation and make it make more sense.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bluetooth and bluez inside Docker</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-11-bluetooth-bluez-inside-docker/</link>
      <pubDate>Fri, 11 Sep 2015 17:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-11-bluetooth-bluez-inside-docker/</guid>
      <description>&lt;p&gt;If you have a Bluetooth device attached to a Linux system, it&amp;rsquo;s
not clear how to attach that device to a Docker container on that
same system.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bluez&lt;/code&gt;, the Bluetooth driver for Linux, uses the &lt;code&gt;D-Bus&lt;/code&gt; to communicate.
This is more complex than a FIFO or socket interface, and thus less
easy to obviously say how it should be mapped into the Docker processs
and file system space.&lt;/p&gt;

&lt;p&gt;The references below show several ways of not doing this task,
and hopefully at some point will get answers on how to do it.&lt;/p&gt;

&lt;p&gt;References for further reading:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/docker/docker/issues/16208&#34;&gt;Bluetooth socket can&amp;rsquo;t be opened inside container&lt;/a&gt;, Docker Github issues 16208&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/28868393/accessing-bluetooth-dongle-from-inside-docker&#34;&gt;Accessing Bluetooth dongle from inside Docker?&lt;/a&gt;, Stack Overflow.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>xml2json as part of a web parsing pipeline</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-10-xml2json/</link>
      <pubDate>Thu, 10 Sep 2015 15:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-10-xml2json/</guid>
      <description>&lt;p&gt;The problem, simply put. You have a messy, real world HTML page;
you want to parse it to pull out some key element of it, to pass
along to some other task; you want that parsing pipeline to be
as compact as possible, because you&amp;rsquo;re running on a small machine,
and you&amp;rsquo;re doing the task frequently, and because the page may
change out from under you at any time.&lt;/p&gt;

&lt;p&gt;Clearly this is not an optimal task to have, but you live with
the web you have, not with the web you want to have.&lt;/p&gt;

&lt;p&gt;My approach to this task is to assemble a pipeline of Unix tools
that are as simple as possible, and that each do one thing pretty
well, and that in combination are all well-refined enough that there
are few surprises.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl&lt;/code&gt; is the workhorse for fetching pages. The invocation
&lt;code&gt;curl -m 15 -s http://example.com&lt;/code&gt; pulls that page from the
net and feeds it to standard output, but times out after 15 seconds
so your pipeline doesn&amp;rsquo;t completely fail if a remote site is down.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tidy&lt;/code&gt; is the first thing I look at when doing data transformation.
&lt;code&gt;tidy -q -asxml 2&amp;gt;/dev/null&lt;/code&gt; takes HTML and converts it, quietly
and uncomplainingly, into XML. It&amp;rsquo;s predictable and pretty fast.
&lt;code&gt;tidy&lt;/code&gt; has been around since the dawn of the web, and it newly
has a &lt;a href=&#34;https://github.com/htacg/tidy-html5&#34;&gt;Github project&lt;/a&gt; and
a &lt;a href=&#34;http://www.html-tidy.org/&#34;&gt;shiny web page&lt;/a&gt; and a support
consortium, so if you want to build from source it&amp;rsquo;s readily possible.&lt;/p&gt;

&lt;p&gt;Given XML, convert it to JSON. Here there are quite a few choices,
depending on which language and which parser you want to start with,
and the dependency tree is deep. I am least satisfied with my alternatives
here, thus the motivation for this writeup. In alphabetical order by
language:&lt;/p&gt;

&lt;p&gt;There is no POSIX standard &lt;code&gt;xml2json&lt;/code&gt; command written in C, alas.&lt;/p&gt;

&lt;p&gt;In C++ there&amp;rsquo;s an &lt;code&gt;xml2json&lt;/code&gt; command and library from Cheedoong Ch&amp;rsquo;ng.
The &lt;a href=&#34;https://github.com/Cheedoong/xml2json&#34;&gt;Github project&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;xml2json is the first carefully written C++ library that converts
XML document to JSON format. It&amp;rsquo;s already been used in the soft
subtitle cross-domain solution at the server-end of Tencent Video
(&lt;a href=&#34;http://v.qq.com&#34;&gt;http://v.qq.com&lt;/a&gt;) and its CDNs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In Dart, see Steve Hamblett&amp;rsquo;s &lt;code&gt;xml2json.dart&lt;/code&gt; library.
The &lt;a href=&#34;https://github.com/shamblett/xml2json&#34;&gt;Github project&lt;/a&gt; includes
a set of unit tests, as well as explicit support for three
conventions (Parker, Badgerfish, and Google Data) for doing
the conversion. The test suites are welcomed.&lt;/p&gt;

&lt;p&gt;In Go, see Darren Elwood (textnode)&amp;rsquo;s &lt;code&gt;xml2json.go&lt;/code&gt;. The
&lt;a href=&#34;https://github.com/textnode/xml2json&#34;&gt;Github project&lt;/a&gt; includes
sample code to parse both RSS and generic XML files.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Transform a stream of XML into a stream of JSON, without requiring
a schema or structs, written in Go (golang.org)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In Node.JS, see &lt;code&gt;node-xml2json&lt;/code&gt; from BugLabs which
converts XML to JSON using &lt;code&gt;node-expat&lt;/code&gt;. You can install it
with &lt;code&gt;npm install xml2json&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In Perl, see Isidro Vila Verde&amp;rsquo;s &lt;code&gt;xml2json.pl&lt;/code&gt;, which has a
&lt;a href=&#34;https://github.com/jvverde/xml2json&#34;&gt;Github project&lt;/a&gt; and is based
on &lt;code&gt;XSLT&lt;/code&gt;. &amp;ldquo;You may need to install some perl modules before using it&amp;rdquo;
is the extent of the install instructions.&lt;/p&gt;

&lt;p&gt;In Python, see the &lt;code&gt;xmlutils&lt;/code&gt; package from Kailash Nadh
and Yigal Lazarev. You can
install it with &lt;code&gt;pip install xmlutils&lt;/code&gt;, or look at the
&lt;a href=&#34;https://github.com/knadh/xmlutils.py&#34;&gt;Github project&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;xmlutils.py is a set of Python utilities for processing xml files
serially for converting them to various formats (SQL, CSV, JSON).
The scripts use ElementTree.iterparse() to iterate through nodes
in an XML document, thus not needing to load the entire DOM into
memory. The scripts can be used to churn through large XML files
(albeit taking long :P) without memory hiccups.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once the original HTML document emerges nicely formatted as JSON,
it&amp;rsquo;s relatively easy to pick out elements from it in most cases.
Two tools to do this are &lt;code&gt;jq&lt;/code&gt; and &lt;code&gt;jp&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;jq&lt;/code&gt;, from Stephen Dolan, is a command-line JSON processor. Now
at version 1.5, it has a &lt;a href=&#34;https://stedolan.github.io/jq/&#34;&gt;web site&lt;/a&gt;
and a &lt;a href=&#34;https://github.com/stedolan/jq&#34;&gt;Github repository&lt;/a&gt;, plus
a handy &lt;a href=&#34;https://jqplay.org/&#34;&gt;online test site&lt;/a&gt; that lets you
do interactive tests. It&amp;rsquo;s written in portable C.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;jq is like sed for JSON data - you can use it to slice and filter
and map and transform structured data with the same ease that sed,
awk, grep and friends let you play with text.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;jp&lt;/code&gt;, from James Saryerwinnie, is a command line version of the JMESPath
query language for JSON, documented at &lt;a href=&#34;http://jmespath.org/&#34;&gt;http://jmespath.org/&lt;/a&gt; . This
is the same query language embedded into the Amazon Web Services
command line (AWS CLI), and it has powerful and compact operators
for extracting elements from a JSON document. There are libraries
in Python, PHP, Javascript, Ruby, Lua, and Go that implement JMESPath.
&lt;code&gt;jp&lt;/code&gt; is written in Go, and has cross-development tools available.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;So that&amp;rsquo;s the environment. It&amp;rsquo;s a bit of a pain to get all of
those tools running on a new bare-metal machine, so my thought
is to put everything into one Docker build and make it straightforward
to wrap everything together. Ideally this will be a fairly
minimalist build, and thus the challenge is to find the set
of dependencies that is small enough.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing a Raspberry Pi OpenVPN with Salt</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-07-managing-raspberry-pi-openvpn-with-salt/</link>
      <pubDate>Mon, 07 Sep 2015 00:45:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-07-managing-raspberry-pi-openvpn-with-salt/</guid>
      <description>&lt;p&gt;When you successfully install something complex, and then you fail
to document one small thing about how you did it, the inclination is
to over-document once you get it working again; thus, this expository
account of managing a set of Raspberry Pi devices using the &amp;ldquo;salt&amp;rdquo;
system. It&amp;rsquo;s not a complete start to finish writeup, just a pointer to
the bits that I had forgotten.&lt;/p&gt;

&lt;p&gt;This is mostly experimental at this point, and the collection of devices
is relatively small. I am pretty convinced that the Pi 2 has plenty of
gas to do more things than most people will try to do, but the original
Pi is slow enough and limited enough to perhaps make this scheme impractical.&lt;/p&gt;

&lt;p&gt;When it works, you have a set of Pi&amp;rsquo;s in various locations, all networked
together into an OpenVPN configuration.&lt;/p&gt;

&lt;p&gt;First to read is &lt;a href=&#34;http://garthwaite.org/virtually-secure-with-openvpn-pillars-and-salt.html&#34;&gt;Virtually secure with openvpn, pillars, and salt&lt;/a&gt; which
goes over the structure and motivation and design for an OpenVPN
network that answers this problem:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How do I use salt to create and install the openvpn and client specific config files for each minion &amp;ndash;on demand?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you follow Dan Garthwaite&amp;rsquo;s design you get a reasonable
approach - when you bring up a new client you get a new key
generated, and on the server you run &lt;code&gt;salt-key -a&lt;/code&gt; to accept the
new one.&lt;/p&gt;

&lt;p&gt;This leaves only the problem of bootstrapping on the Pi&amp;rsquo;s themselves.
The version of &amp;lsquo;salt-minion&amp;rsquo; that&amp;rsquo;s available from the default
Debian system is very ancient, so that&amp;rsquo;s useless. What you want
can be found at
&lt;a href=&#34;http://servernetworktech.com/2014/05/setup-debian-saltstack-minion-single-command/&#34;&gt;Setup a Debian Saltstack minion with a single command&lt;/a&gt;
on the Server Network Tech blog; it brings in a new Debian
source list from Salt so that the bootstrap is easier.&lt;/p&gt;

&lt;p&gt;(I realize this isn&amp;rsquo;t a start to finish tutorial, just a few points
of clarification for bringing up new nodes once you have the whole
Salt environment running; sorry about that.)&lt;/p&gt;

&lt;p&gt;The one thing I have left to do to make the setup more airtight is
to automate the process of locking down the sshd_config so that &amp;ldquo;root&amp;rdquo;
and &amp;ldquo;pi&amp;rdquo; can&amp;rsquo;t login from far away, so that I can keep their default
passwords but restrict access to console access only.&lt;/p&gt;

&lt;p&gt;Not mentioned here is that these Pi configuraiton are all on top of
the Hypriot setup so that they can (and do) run Docker. Though Docker
on the original Pi with salt-minion running doesn&amp;rsquo;t leave a ton of
space spare for actual work to be done.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep web</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-04-deep-web/</link>
      <pubDate>Fri, 04 Sep 2015 08:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-04-deep-web/</guid>
      <description>&lt;p&gt;I think this whole &amp;ldquo;deep web&amp;rdquo; thing is mis-stated for the most part.&lt;/p&gt;

&lt;p&gt;Since forever there have been Internet-based services that have
been unreachable to the public unless you had some kind of specialized
access. Private internal networks are the norm at companies of any
size. Calling that the &amp;ldquo;hidden web&amp;rdquo;, &amp;ldquo;deep web&amp;rdquo; is non-sensical in
that it&amp;rsquo;s not really hidden to anyone who has proper access to it
- it&amp;rsquo;s just that you (and Google) don&amp;rsquo;t and won&amp;rsquo;t have that access.&lt;/p&gt;

&lt;p&gt;Furthermore there are enormous numbers of home networks behind NAT
devices where there are home servers that are either accidentally
or deliberately off-limits. If you want to get that data onto the
public network you actually have to do some work to make it happen.&lt;/p&gt;

&lt;p&gt;I even have a batch of test services running on my laptop that don&amp;rsquo;t
have any public presence at all beyond my keyboard.&lt;/p&gt;

&lt;p&gt;There happen to be parts of the &amp;ldquo;secret web&amp;rdquo;, &amp;ldquo;deep web&amp;rdquo; etc where
there&amp;rsquo;s some naughty content, but for the most part the Internet
architecture makes it completely normal to have hosts that only
have partial routing to the world, and most of the time that&amp;rsquo;s
completely innocuous.&lt;/p&gt;

&lt;p&gt;(As originally posted to Quora to answer the question, &amp;ldquo;How is the
Deep Web so huge?)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>