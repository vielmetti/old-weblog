<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vacuum weblog from Edward Vielmetti</title>
    <link>http://vielmetti.github.io/</link>
    <description>Recent content on Vacuum weblog from Edward Vielmetti</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Sep 2015 15:00:00 -0400</lastBuildDate>
    <atom:link href="http://vielmetti.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>xml2json as part of a web parsing pipeline</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-10-xml2json/</link>
      <pubDate>Thu, 10 Sep 2015 15:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-10-xml2json/</guid>
      <description>&lt;p&gt;The problem, simply put. You have a messy, real world HTML page;
you want to parse it to pull out some key element of it, to pass
along to some other task; you want that parsing pipeline to be
as compact as possible, because you&amp;rsquo;re running on a small machine,
and you&amp;rsquo;re doing the task frequently, and because the page may
change out from under you at any time.&lt;/p&gt;

&lt;p&gt;Clearly this is not an optimal task to have, but you live with
the web you have, not with the web you want to have.&lt;/p&gt;

&lt;p&gt;My approach to this task is to assemble a pipeline of Unix tools
that are as simple as possible, and that each do one thing pretty
well, and that in combination are all well-refined enough that there
are few surprises.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl&lt;/code&gt; is the workhorse for fetching pages. The invocation
&lt;code&gt;curl -m 15 -s http://example.com&lt;/code&gt; pulls that page from the
net and feeds it to standard output, but times out after 15 seconds
so your pipeline doesn&amp;rsquo;t completely fail if a remote site is down.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tidy&lt;/code&gt; is the first thing I look at when doing data transformation.
&lt;code&gt;tidy -q -asxml 2&amp;gt;/dev/null&lt;/code&gt; takes HTML and converts it, quietly
and uncomplainingly, into XML. It&amp;rsquo;s predictable and pretty fast.
&lt;code&gt;tidy&lt;/code&gt; has been around since the dawn of the web, and it newly
has a &lt;a href=&#34;https://github.com/htacg/tidy-html5&#34;&gt;Github project&lt;/a&gt; and
a &lt;a href=&#34;http://www.html-tidy.org/&#34;&gt;shiny web page&lt;/a&gt; and a support
consortium, so if you want to build from source it&amp;rsquo;s readily possible.&lt;/p&gt;

&lt;p&gt;Given XML, convert it to JSON. Here there are quite a few choices,
depending on which language and which parser you want to start with,
and the dependency tree is deep. I am least satisfied with my alternatives
here, thus the motivation for this writeup. In alphabetical order by
language:&lt;/p&gt;

&lt;p&gt;There is no POSIX standard &lt;code&gt;xml2json&lt;/code&gt; command written in C, alas.&lt;/p&gt;

&lt;p&gt;In C++ there&amp;rsquo;s an &lt;code&gt;xml2json&lt;/code&gt; command and library from Cheedoong Ch&amp;rsquo;ng.
The &lt;a href=&#34;https://github.com/Cheedoong/xml2json&#34;&gt;Github project&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;xml2json is the first carefully written C++ library that converts
XML document to JSON format. It&amp;rsquo;s already been used in the soft
subtitle cross-domain solution at the server-end of Tencent Video
(&lt;a href=&#34;http://v.qq.com&#34;&gt;http://v.qq.com&lt;/a&gt;) and its CDNs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In Dart, see Steve Hamblett&amp;rsquo;s &lt;code&gt;xml2json.dart&lt;/code&gt; library.
The &lt;a href=&#34;https://github.com/shamblett/xml2json&#34;&gt;Github project&lt;/a&gt; includes
a set of unit tests, as well as explicit support for three
conventions (Parker, Badgerfish, and Google Data) for doing
the conversion. The test suites are welcomed.&lt;/p&gt;

&lt;p&gt;In Go, see Darren Elwood (textnode)&amp;rsquo;s &lt;code&gt;xml2json.go&lt;/code&gt;. The
&lt;a href=&#34;https://github.com/textnode/xml2json&#34;&gt;Github project&lt;/a&gt; includes
sample code to parse both RSS and generic XML files.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Transform a stream of XML into a stream of JSON, without requiring
a schema or structs, written in Go (golang.org)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In Node.JS, see &lt;code&gt;node-xml2json&lt;/code&gt; from BugLabs which
converts XML to JSON using &lt;code&gt;node-expat&lt;/code&gt;. You can install it
with &lt;code&gt;npm install xml2json&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In Perl, see Isidro Vila Verde&amp;rsquo;s &lt;code&gt;xml2json.pl&lt;/code&gt;, which has a
&lt;a href=&#34;https://github.com/jvverde/xml2json&#34;&gt;Github project&lt;/a&gt; and is based
on &lt;code&gt;XSLT&lt;/code&gt;. &amp;ldquo;You may need to install some perl modules before using it&amp;rdquo;
is the extent of the install instructions.&lt;/p&gt;

&lt;p&gt;In Python, see the &lt;code&gt;xmlutils&lt;/code&gt; package from Kailash Nadh
and Yigal Lazarev. You can
install it with &lt;code&gt;pip install xmlutils&lt;/code&gt;, or look at the
&lt;a href=&#34;https://github.com/knadh/xmlutils.py&#34;&gt;Github project&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;xmlutils.py is a set of Python utilities for processing xml files
serially for converting them to various formats (SQL, CSV, JSON).
The scripts use ElementTree.iterparse() to iterate through nodes
in an XML document, thus not needing to load the entire DOM into
memory. The scripts can be used to churn through large XML files
(albeit taking long :P) without memory hiccups.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once the original HTML document emerges nicely formatted as JSON,
it&amp;rsquo;s relatively easy to pick out elements from it in most cases.
Two tools to do this are &lt;code&gt;jq&lt;/code&gt; and &lt;code&gt;jp&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;jq&lt;/code&gt;, from Stephen Dolan, is a command-line JSON processor. Now
at version 1.5, it has a &lt;a href=&#34;https://stedolan.github.io/jq/&#34;&gt;web site&lt;/a&gt;
and a &lt;a href=&#34;https://github.com/stedolan/jq&#34;&gt;Github repository&lt;/a&gt;, plus
a handy &lt;a href=&#34;https://jqplay.org/&#34;&gt;online test site&lt;/a&gt; that lets you
do interactive tests. It&amp;rsquo;s written in portable C.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;jq is like sed for JSON data - you can use it to slice and filter
and map and transform structured data with the same ease that sed,
awk, grep and friends let you play with text.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;jp&lt;/code&gt;, from James Saryerwinnie, is a command line version of the JMESPath
query language for JSON, documented at &lt;a href=&#34;http://jmespath.org/&#34;&gt;http://jmespath.org/&lt;/a&gt; . This
is the same query language embedded into the Amazon Web Services
command line (AWS CLI), and it has powerful and compact operators
for extracting elements from a JSON document. There are libraries
in Python, PHP, Javascript, Ruby, Lua, and Go that implement JMESPath.
&lt;code&gt;jp&lt;/code&gt; is written in Go, and has cross-development tools available.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;So that&amp;rsquo;s the environment. It&amp;rsquo;s a bit of a pain to get all of
those tools running on a new bare-metal machine, so my thought
is to put everything into one Docker build and make it straightforward
to wrap everything together. Ideally this will be a fairly
minimalist build, and thus the challenge is to find the set
of dependencies that is small enough.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing a Raspberry Pi OpenVPN with Salt</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-07-managing-raspberry-pi-openvpn-with-salt/</link>
      <pubDate>Mon, 07 Sep 2015 00:45:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-07-managing-raspberry-pi-openvpn-with-salt/</guid>
      <description>&lt;p&gt;When you successfully install something complex, and then you fail
to document one small thing about how you did it, the inclination is
to over-document once you get it working again; thus, this expository
account of managing a set of Raspberry Pi devices using the &amp;ldquo;salt&amp;rdquo;
system. It&amp;rsquo;s not a complete start to finish writeup, just a pointer to
the bits that I had forgotten.&lt;/p&gt;

&lt;p&gt;This is mostly experimental at this point, and the collection of devices
is relatively small. I am pretty convinced that the Pi 2 has plenty of
gas to do more things than most people will try to do, but the original
Pi is slow enough and limited enough to perhaps make this scheme impractical.&lt;/p&gt;

&lt;p&gt;When it works, you have a set of Pi&amp;rsquo;s in various locations, all networked
together into an OpenVPN configuration.&lt;/p&gt;

&lt;p&gt;First to read is &lt;a href=&#34;http://garthwaite.org/virtually-secure-with-openvpn-pillars-and-salt.html&#34;&gt;Virtually secure with openvpn, pillars, and salt&lt;/a&gt; which
goes over the structure and motivation and design for an OpenVPN
network that answers this problem:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How do I use salt to create and install the openvpn and client specific config files for each minion &amp;ndash;on demand?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you follow Dan Garthwaite&amp;rsquo;s design you get a reasonable
approach - when you bring up a new client you get a new key
generated, and on the server you run &lt;code&gt;salt-key -a&lt;/code&gt; to accept the
new one.&lt;/p&gt;

&lt;p&gt;This leaves only the problem of bootstrapping on the Pi&amp;rsquo;s themselves.
The version of &amp;lsquo;salt-minion&amp;rsquo; that&amp;rsquo;s available from the default
Debian system is very ancient, so that&amp;rsquo;s useless. What you want
can be found at
&lt;a href=&#34;http://servernetworktech.com/2014/05/setup-debian-saltstack-minion-single-command/&#34;&gt;Setup a Debian Saltstack minion with a single command&lt;/a&gt;
on the Server Network Tech blog; it brings in a new Debian
source list from Salt so that the bootstrap is easier.&lt;/p&gt;

&lt;p&gt;(I realize this isn&amp;rsquo;t a start to finish tutorial, just a few points
of clarification for bringing up new nodes once you have the whole
Salt environment running; sorry about that.)&lt;/p&gt;

&lt;p&gt;The one thing I have left to do to make the setup more airtight is
to automate the process of locking down the sshd_config so that &amp;ldquo;root&amp;rdquo;
and &amp;ldquo;pi&amp;rdquo; can&amp;rsquo;t login from far away, so that I can keep their default
passwords but restrict access to console access only.&lt;/p&gt;

&lt;p&gt;Not mentioned here is that these Pi configuraiton are all on top of
the Hypriot setup so that they can (and do) run Docker. Though Docker
on the original Pi with salt-minion running doesn&amp;rsquo;t leave a ton of
space spare for actual work to be done.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep web</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-04-deep-web/</link>
      <pubDate>Fri, 04 Sep 2015 08:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-04-deep-web/</guid>
      <description>&lt;p&gt;I think this whole &amp;ldquo;deep web&amp;rdquo; thing is mis-stated for the most part.&lt;/p&gt;

&lt;p&gt;Since forever there have been Internet-based services that have
been unreachable to the public unless you had some kind of specialized
access. Private internal networks are the norm at companies of any
size. Calling that the &amp;ldquo;hidden web&amp;rdquo;, &amp;ldquo;deep web&amp;rdquo; is non-sensical in
that it&amp;rsquo;s not really hidden to anyone who has proper access to it
- it&amp;rsquo;s just that you (and Google) don&amp;rsquo;t and won&amp;rsquo;t have that access.&lt;/p&gt;

&lt;p&gt;Furthermore there are enormous numbers of home networks behind NAT
devices where there are home servers that are either accidentally
or deliberately off-limits. If you want to get that data onto the
public network you actually have to do some work to make it happen.&lt;/p&gt;

&lt;p&gt;I even have a batch of test services running on my laptop that don&amp;rsquo;t
have any public presence at all beyond my keyboard.&lt;/p&gt;

&lt;p&gt;There happen to be parts of the &amp;ldquo;secret web&amp;rdquo;, &amp;ldquo;deep web&amp;rdquo; etc where
there&amp;rsquo;s some naughty content, but for the most part the Internet
architecture makes it completely normal to have hosts that only
have partial routing to the world, and most of the time that&amp;rsquo;s
completely innocuous.&lt;/p&gt;

&lt;p&gt;(As originally posted to Quora to answer the question, &amp;ldquo;How is the
Deep Web so huge?)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Over the top networks, a history of building new systems on the wreckage of the old</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-30-over-the-top-networks/</link>
      <pubDate>Sun, 30 Aug 2015 14:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-30-over-the-top-networks/</guid>
      <description>&lt;p&gt;This is a copy of a proposal for a talk at the Radical/Networks conference.
&lt;a href=&#34;https://github.com/chootka/radical-networks/issues/27&#34;&gt;The original proposal&lt;/a&gt;
is on Github.
The event runs October 24th and 25th (Saturday +
Sunday) from 10a - 7p at NYU Poly in Brooklyn, NY.&lt;/p&gt;

&lt;p&gt;Title: Over the top networks, a history of building new systems on the wreckage of the old&lt;/p&gt;

&lt;p&gt;New networks often feed off of old ones, in ways that characteristically
draw energy from aspects of those networks that are easy to take
over and hard to defend. In this talk, I&amp;rsquo;ll look at the waves of
creative destruction that are unleashed when network developers
find existing infrastructure that can be exploited for new ends,
and the ways that commoditized networks fight back to avoid being
turned into dumb pipes.&lt;/p&gt;

&lt;p&gt;The talk will look at the history and the future of these overlay
or over-the-top networks, going back three decades for stories of
networks like Usenet, electronic mail, and payments networks and
how they started out parasitizing existing older networks only be
overtopped by other interests as file sharing, security, and identity
layers of newer networks. I&amp;rsquo;ll look at rules for developers of
radical advances in networks and guidelines to avoid the pitfalls
of unexpected dependencies and hidden traps.&lt;/p&gt;

&lt;p&gt;Edward Vielmetti has been working on the Internet since 1985 from
Ann Arbor, Michigan. His background includes work on the early
commercialization and privatization of the Internet. Previous
conference presentations include &amp;ldquo;WYSSA means all my love, darling:
A social history of the Internet from the carrier pigeon to Antarctic
morse code&amp;rdquo; (UPA 2006) and &amp;ldquo;Perils and Pitfalls of Practical
Cybercommerce&amp;rdquo; (ARABANK 1996, Dubai, United Arab Emirates).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terabytes, at least in theory</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-29-terabytes-at-least-in-theory/</link>
      <pubDate>Sat, 29 Aug 2015 21:30:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-29-terabytes-at-least-in-theory/</guid>
      <description>&lt;p&gt;I have a new terabyte drive. It&amp;rsquo;s impossibly small, impossibly
cheap, and just fast enough to accomplish astounding backup tasks
in what seems like not enough time.&lt;/p&gt;

&lt;p&gt;It is, however, a mass storage device that depends on rotating
magnetic media, and as such it&amp;rsquo;s obsolete.&lt;/p&gt;

&lt;p&gt;The drive cost less than $50. For about 10 times that right now,
I can buy another terabyte drive that is solid state rather than
spinning oxide storage. The bet that I have is that in three years,
the cost of a terabyte of SSD drive will have come down so far -
perhaps to $100 - that no one will have any interest in selling me
a 1T drive that isn&amp;rsquo;t SSD.&lt;/p&gt;

&lt;p&gt;My frame of reference for &amp;ldquo;what does har drive storage cost&amp;rdquo; dates
back as far as the 10 megabyte, $1000 drive, which I really wanted
and which I never bought for my own self. And then back a little bit
further, when a 5 megabyte Davong drive was the bee&amp;rsquo;s knees.&lt;/p&gt;

&lt;p&gt;What do you do with a terabyte? Lots of backups.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RRDTool v1.5.4 on Raspberry Pi</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-26-rrdtool-on-raspberry-pi/</link>
      <pubDate>Wed, 26 Aug 2015 17:15:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-26-rrdtool-on-raspberry-pi/</guid>
      <description>&lt;p&gt;The second in a series of efforts to get a suitable database for
the task.&lt;/p&gt;

&lt;p&gt;The challenge is that there&amp;rsquo;s a lot of data out there that changes
all the time, and you want to gather it, make sure it&amp;rsquo;s sane, squirrel
it away for later analysis, put together a pretty chart so that you
can spot check it, feed it into other systems not yet fully dreamed
of to cause actions, and so on. It&amp;rsquo;s the &amp;ldquo;big data&amp;rdquo; problem in that
you have sensors that throw off a lot of data exhaust, and it&amp;rsquo;s the
&amp;ldquo;internet of things&amp;rdquo; problem because mostly you&amp;rsquo;re looking at things
with data and not say imagery or lots of freeform text.&lt;/p&gt;

&lt;p&gt;Phase one of this effort was wrestling MongoDB into service, which
ended up being only a handful of lines of code; but the concern expressed
was the Mongo though fun and easy for small projects gets messy for
big ones, and there&amp;rsquo;s good reason not to succumb to its siren song.
The other reason to stop using Mongo when I did was that it really
doesn&amp;rsquo;t love the Raspberry Pi architecture. Since there were only really
a very few lines of code, and since that code works on my Mac, no time
lost - but time to move on.&lt;/p&gt;

&lt;p&gt;Phase two is a look at RRDtool, a &amp;ldquo;round robin&amp;rdquo; database very often used
for network management efforts. One of the nice things about a round robin
database is that the database itself is at its maximum size when you
create it, and it doesn&amp;rsquo;t grow - rather, it cycles out the old data
at whatever time resolution you want to get rid of it. With that in
mind you can have a single database with 10 years worth of daily averages
and a whole year of minute-by-minute data and it hardly takes up any
room at all - perfect for the sort of &amp;ldquo;data logger&amp;rdquo; applications that
I find myself in the middle of.&lt;/p&gt;

&lt;p&gt;If you do &lt;code&gt;apt-get install rrdtool&lt;/code&gt; on the Pi you get a working
copy of RRDtool 1.4.7, compiled in 2012. That&amp;rsquo;s pretty long in the
tooth, so I decided to pull down v1.5.4 from the
&lt;a href=&#34;http://oss.oetiker.ch/rrdtool/pub/?M=D&#34;&gt;RRDtool download area&lt;/a&gt;. The release notes note
dependencies as follows&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get install libpango1.0-dev libxml2-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and once you have those pulled down the build is not fast but not
difficult.&lt;/p&gt;

&lt;p&gt;Pushing data into RRDtool is as simple as a shell script run out of
cron. The first data point I&amp;rsquo;m graphing is &amp;ldquo;total spaces free in the
Ann Arbor DDA parking garages&amp;rdquo;, and I&amp;rsquo;m collecting that on two machines -
my Mac when it&amp;rsquo;s running, and the Pi at all times. A second graph
I want to capture is some really basic weather information, and a
third is inside-the-machine environmental sensing starting with the
system temperature of the Pi.&lt;/p&gt;

&lt;p&gt;It was really useful to make the first round of this be Mongo, because
with Mongo you really don&amp;rsquo;t have to decide anything when you set things up -
just dump in raw JSON documents and hope for the best.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://vielmetti.github.io/images/parking.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;RRDtool graph of available parking in downtown Ann Arbor&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First pass at MongoDB</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-26-first-pass-at-mongodb/</link>
      <pubDate>Wed, 26 Aug 2015 00:40:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-26-first-pass-at-mongodb/</guid>
      <description>&lt;p&gt;Somehow I never learned SQL along the way. At various times I have
learned enough of the database lingo (inner join, outer join etc) to
know where I missing things, and I have a lot of experience with common
Unix tools (grep, sed, awk) and some newfangled tools (jq, csvkit) to
be able to turn a big pile of data into a smaller pile of data. But
I don&amp;rsquo;t have database chops per se right now, and sometimes that&amp;rsquo;s a
limiting factor on projects when the data sets are too big.&lt;/p&gt;

&lt;p&gt;JSON formatted data seems to have won the data serialization wars, so
that at least transforms the problem on input; with jq (now in release 1.5)
you can take a stream of JSON formatted data that&amp;rsquo;s almost what you want
and mangle it until it&amp;rsquo;s really what you want. Lots of web APIs spit out
some flavor of JSON, and even if it&amp;rsquo;s not precisely formatted to be ready
for input into something else, at least there&amp;rsquo;s a straightforward way to
munge it into something better.&lt;/p&gt;

&lt;p&gt;That leaves the task of dumping these JSON blobs into a database, in
such a way that you can relatively easily get something back out of the
database that answers what you are really trying to answer again. Here
the world splits into two pieces: the SQL databases that have been taught
how to deal with JSON (specifically PostgreSQL, as of version 9.3), and the
so-called NoSQL databases (specifically MongoDB).&lt;/p&gt;

&lt;p&gt;Even Mongo knows that &lt;a href=&#34;http://blog.mongodb.org/post/62899600960/scaling-advice-from-mongohq&#34;&gt;scaling MongoDB is hard&lt;/a&gt;, though
they frame it as the first 100 gigabytes of data stored in the system is
easy. Fortunately, the first set of tasks I have to throw at this particular
problem space are relatively small, databases measured in the 10s of megabytes.
Pretty much anything will work - even grep - on this size data. The
challenge becomes more of a how fast can you get something real running
that has relatively simple queries, relatively performant results, and
that can run on inexpensive hardware.&lt;/p&gt;

&lt;p&gt;So I learned just enough Mongo to handle one tiny task: connect to an API,
download some data periodically, clean it up enough so that it&amp;rsquo;s regular,
and store it away; and then construct queries that are conceptually simple
but that answer real questions. From there, do the ad hoc queries that prove
to yourself that you have the right data, and then bootstrap some web
app on top of all that to show off what you&amp;rsquo;ve done.&lt;/p&gt;

&lt;p&gt;The results here accomplish a relatively complex task with a handful of
lines of code. An API from a local agency is queried that has real time data.
That data gets cleaned up and standardized and then inserted into a Mongo
database. It&amp;rsquo;s all of 6 lines of code.&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/vielmetti/85d5fba07b98e552c912.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Listening to Radio Havana Cuba via a shortwave software defined radio</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-24-radio-havana-cuba/</link>
      <pubDate>Mon, 24 Aug 2015 22:45:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-24-radio-havana-cuba/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m listening to Radio Havana Cuba via a shortwave listening post set up
with a software defined radio. The system, &lt;a href=&#34;http://blerp2.dyndns.org:8901/&#34;&gt;Great Lakes Listening Post&lt;/a&gt;,
has four separate radios covering the 80, 40, 30, and 20 meter bands, all
fed into the &lt;a href=&#34;http://www.websdr.org&#34;&gt;WebSDR&lt;/a&gt; software from PA3FWM. It&amp;rsquo;s
located in Michigan, and thus approximates the radio reception I&amp;rsquo;d get from
my own receiver.&lt;/p&gt;

&lt;p&gt;R Havana Cuba is a good station to listen to. There&amp;rsquo;s interesting political
commentary, fun music, and the occasional show about shortwave listening.
The signal is usually strong and the frequency (6000 kHz) is easy to remember.
Most international braodcasters have refocused their attention on Internet
based broadcasts, but R Havana Cuba continues on with its shortwave service.
My Sony ICF-2010 set from the 1980s still has a Cold War era present to that
frequency.&lt;/p&gt;

&lt;p&gt;The biggest advantage to web-based software radio is the ability to pick up
stations from listening posts around the world. The WebSDR platform has stations
with good antennas around the world, and they each allow multiple listeners
to tune in to different stations without any of them monopolizing the receiver.
Most web-based SDR systems focus on shortwave and HF amateur bands, but every
so often you find a system that can tune domestic medium wave (AM) and
long wave bands, which gives you a chance to hear programs that were
never anticipated to be listened to from half way around the world.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://vielmetti.github.io/images/R-Havana-Cuba-WebSDR.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Radio Havana Cuba on Great Lakes Listening Post WebSDR&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Discussion on &lt;a href=&#34;https://www.facebook.com/edward.vielmetti/posts/10105619636684853&#34;&gt;Facebook&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer dinner</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-24-summer-dinner/</link>
      <pubDate>Mon, 24 Aug 2015 11:55:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-24-summer-dinner/</guid>
      <description>&lt;p&gt;Dinner: roasted red peppers, roasted green beans, boiled potatoes,
cool and crunchy red pepper and yellow tomato salad with balsamic,
kale, and either eggs or bratwurst depending on your preferences.
I really prefer cooking in the summertime!&lt;/p&gt;

&lt;p&gt;Discussion on &lt;a href=&#34;https://www.facebook.com/edward.vielmetti/posts/10105614819932663?notif_t=like&#34;&gt;Facebook&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GQRX based radio scanner with gqrx-scan</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-23-gqrx-scanner/</link>
      <pubDate>Sun, 23 Aug 2015 11:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-23-gqrx-scanner/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/khaytsus/gqrx-scan&#34;&gt;gqrx-scan&lt;/a&gt; is a Perl script
that drives the remote control of the &lt;a href=&#34;http://gqrx.dk/&#34;&gt;gqrx&lt;/a&gt; software
defined radio receiver to turn it into a scanner and automated recording tool.&lt;/p&gt;

&lt;p&gt;The GQRX &lt;a href=&#34;http://gqrx.dk/doc/remote-control&#34;&gt;remote control protocol&lt;/a&gt; is
well documented and fairly simple, which means that the bulk of the
Perl script has to do with setting and getting options. You can drive
your scanner from the GQRX configuration file, or you can monitor
a specific band or range of frequencies. It will even toggle the record
mode in GQRX so that you can save what you&amp;rsquo;re listening to a file.&lt;/p&gt;

&lt;p&gt;Because the system is driven from a file, I would like to see more
example files in the distribution - e.g. one that had all of the
GMRS/FRS frequencies already typed in so that you could start scanning
those bands immediately. But that&amp;rsquo;s a quibble, and more to the point
that&amp;rsquo;s something that I can do next when the next time I&amp;rsquo;m ready to
contribute to the project.&lt;/p&gt;

&lt;p&gt;Credit goes to Github user &lt;a href=&#34;https://github.com/khaytsus&#34;&gt;khaytsus&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Github Issues and Waffle.io as a personal kanban board</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-22-github-waffle-kanban-board/</link>
      <pubDate>Sat, 22 Aug 2015 23:55:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-22-github-waffle-kanban-board/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been using Github issues to track my personal todo list for quite some
time now, and since November I&amp;rsquo;ve been using Waffle.io to help that process.&lt;/p&gt;

&lt;p&gt;Waffle in this use is a view of a series of queues in my todo list. Most everything
that I&amp;rsquo;m not doing now lives in the &amp;ldquo;backlog&amp;rdquo; queue, which I try to keep to
less than 100 just to keep myself sane. (At more than 100 there&amp;rsquo;s usually a lot
of duplicates; at less than 100 I&amp;rsquo;m probably not acknowledging that there&amp;rsquo;s
something to be done). The &amp;ldquo;Ready&amp;rdquo; and &amp;ldquo;In Progress&amp;rdquo; queues hold no more than
5 tasks apiece, since I want to keep everything in that state visible on
the screen. Finally the &amp;ldquo;Done&amp;rdquo; queue reflects how much work there is that
actually got accomplished over the last week, and I&amp;rsquo;d love to say that was
hovering at about 50 tasks a week though right now it&amp;rsquo;s only at 34.&lt;/p&gt;

&lt;p&gt;Github issues are more than capable of handling a personal todo list - indeed
it has a lot of features that if you tried to use all of them you&amp;rsquo;d quickly
find that too much of your time was perfecting your lists and not enough
time was spent doing things. So the challenge is coming up with a simple
configuration of the system that doesn&amp;rsquo;t ignore what might be done, but that
focuses on what really needs to be done.&lt;/p&gt;

&lt;p&gt;The inspriation for all of this is Jim Benson&amp;rsquo;s &amp;ldquo;Personal Kanban&amp;rdquo;, and his
observation that really the task simplifies down to two rules:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Limit work in progress&lt;/li&gt;
&lt;li&gt;Visualize your work&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rest is merely commentary.&lt;/p&gt;

&lt;p&gt;(Now I get to drag task 3568, &amp;ldquo;Write up use of Waffle for Vacuum blog&amp;rdquo;, into the
&amp;ldquo;Done&amp;rdquo; category, which is kinetically very satisfactory.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>August 2015 Farmers Market report</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-22-august-farmers-market-report/</link>
      <pubDate>Sat, 22 Aug 2015 16:00:01 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-22-august-farmers-market-report/</guid>
      <description>&lt;p&gt;At Ann Arbor Farmers Market:&lt;/p&gt;

&lt;p&gt;Red Haven peaches 🍑, potatoes 🍠, kale, green onions, tomatoes, red peppers,
and a cherry snow cone for my shopping companion.&lt;/p&gt;

&lt;p&gt;(Testing to see how the emoji comes out; I didn&amp;rsquo;t see a kale emoji.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ashley and Madison, Ann Arbor</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-22-ashley-madison-ann-arbor/</link>
      <pubDate>Sat, 22 Aug 2015 16:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-22-ashley-madison-ann-arbor/</guid>
      <description>&lt;p&gt;At the corner of Ashley and Madison in Ann Arbor is the Washtenaw Dairy.&lt;/p&gt;

&lt;p&gt;Ashley Street is named after &amp;ldquo;Big Jim&amp;rdquo; Ashley, an Ohio congressman and
promoter and builder of what became the Ann Arbor Railroad.&lt;/p&gt;

&lt;p&gt;Madison Street is presumably named after President James Madison, as a number
of downtown streets have presidential names.&lt;/p&gt;

&lt;p&gt;There is an Ashley, Michigan in Gratiot County between Owosso and Alma,
on the route of the railroad. Madison Heights, MI is just north of Royal Oak.&lt;/p&gt;

&lt;p&gt;More discussion of these pertinent facts is on &lt;a href=&#34;https://www.facebook.com/edward.vielmetti/posts/10105610388353583&#34;&gt;Facebook&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Liquid contact indicator tripped</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-19-liquid-contact-indicator-tripped/</link>
      <pubDate>Wed, 19 Aug 2015 16:30:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-19-liquid-contact-indicator-tripped/</guid>
      <description>&lt;p&gt;Fortunately, a lot of parts were replacable, and the ones that didn&amp;rsquo;t
need to be replaced were returned intact with all of their data on them.
Back in business.&lt;/p&gt;

&lt;p&gt;I am happy to have paper notebooks and to know how to use them, and to
also have good enough backup systems that I didn&amp;rsquo;t panic. There are
a few very small things I need to learn, mostly about Github and how
to manage subprojects.&lt;/p&gt;

&lt;p&gt;I won&amp;rsquo;t be drinking coffee while I type any time soon. I wonder how that
will affect my work habits. It is perfectly OK to drink coffee and write
or draw or doodle on paper, though.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ann Arbor City Council Preview for 17 August 2015</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-15-a2council-preview/</link>
      <pubDate>Sat, 15 Aug 2015 15:30:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-15-a2council-preview/</guid>
      <description>&lt;p&gt;Ann Arbor City Council meets at 7:00 p.m. on Monday, 17 August 2015.
A preview of the agenda is on Arborwiki, complete with a map, on the
tag &lt;a href=&#34;https://localwiki.org/ann-arbor/tags/20150817ccmtg&#34;&gt;2015-08-17CCMTG&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some highlights to note:&lt;/p&gt;

&lt;p&gt;The consent agenda includes $36,412 for the purchase of 21 Taser weapons.
This is a sole-source purchase, to replace weapons purchased in 2003.&lt;/p&gt;

&lt;p&gt;Also on the consent agenda is $83,000 for a sole-source purchase to
renew the CityWorks license, which is used for asset tracking and
maintenance of city owned resources. Of that amount, $20,000 will be
billed back to the Washtenaw County Water Resources Commissioner.&lt;/p&gt;

&lt;p&gt;$110,000 would go for the continuation of CLEMIS Public Safety Information
Sharing Services for the Ann Arbor Police Department for FY2016. CLEMIS
runs the dispatch system for police and allows &amp;ldquo;information sharing&amp;rdquo;
with other departments. It&amp;rsquo;s run by Oakland County.&lt;/p&gt;

&lt;p&gt;HathiTrust has digitized many decades of Ann Arbor City Council minutes,
and it it proposed that the city approve publication of those minutes through
HathiTrust by clearing the copyright on those materials.&lt;/p&gt;

&lt;p&gt;Public hearings include one on deer management, and there&amp;rsquo;s a proposal
on the agenda to spend $90,000 on some combination of a deer cull by
sharpshooters and sterlization of deer. Expect this to be a long public
hearing.&lt;/p&gt;

&lt;p&gt;In development news, there&amp;rsquo;s a proposed condominium development on N Main St
adjacent to North Main Park, and hearings on proposed annexations on Nixon
Road and on Ann Arbor - Saline Road.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re following the footing drain disconnect program closely, you&amp;rsquo;ll
note that there&amp;rsquo;s a proposal to lend the builder of 618 South Main
footing drain disconnect credits to allow for full occupancy of that building.&lt;/p&gt;

&lt;p&gt;More of course at the full agenda. See you on the site plan barricades
(er, see you at council).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>