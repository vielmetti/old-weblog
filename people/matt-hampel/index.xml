<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matt Hampel on tracker</title>
    <link>http://vielmetti.github.io/people/matt-hampel/</link>
    <description>Recent content in Matt Hampel on tracker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Mar 2015 11:14:50 -0400</lastBuildDate>
    <atom:link href="http://vielmetti.github.io/people/matt-hampel/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Cleaning polygons in R and Shapely</title>
      <link>http://vielmetti.github.io/post/2015/2015-03-04-cleaning-polygons-in-r-and-shapely/</link>
      <pubDate>Wed, 04 Mar 2015 11:14:50 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-03-04-cleaning-polygons-in-r-and-shapely/</guid>
      <description>&lt;p&gt;The project of the week has been an effort to generate a set of maps - 2935 of them, in fact. The source data is from the Department of Energy, listing a set of utility companies and the counties they cover; the goal is to have a boundary map generated for each utility company. I know it won&#39;t be a perfect boundary (most utility companies have at least one county that they only provide service to part of), but as a start it&#39;s a pretty good one.&lt;/p&gt;

&lt;p&gt;The DOE data is based on their Form EIA-861, and I&#39;m working from the 2013 data set. See &lt;a href=&#34;http://www.eia.gov/electricity/data/eia861/&#34;&gt;Electric power sales, revenue, and energy efficiency Form EIA-861 detailed data files&lt;/a&gt; if you want to get your own files. It&#39;s provided as Excel (XLSX) files, so you&#39;ll need something compatible to read them; I ended up using &lt;a href=&#34;http://csvkit.readthedocs.org/en/latest/scripts/in2csv.html&#34;&gt;in2csv from csvkit&lt;/a&gt; as the first part of the pipeline to break these apart.&lt;/p&gt;

&lt;p&gt;You get lines from the provided file that look like this:&lt;/p&gt;

&lt;pre&gt;
Data Year,Utility Number,Utility Name,State,County
2013,5109,DTE Electric Company,MI,Shiawassee
2013,5109,DTE Electric Company,MI,St Clair
2013,5109,DTE Electric Company,MI,Tuscola
2013,5109,DTE Electric Company,MI,Washtenaw
2013,5109,DTE Electric Company,MI,Wayne
&lt;/pre&gt;

&lt;p&gt;The next challenge is to pull a map for each county served. To do this, convert the (State,County) list to a FIPS code, using a table like this one from ORNL called &lt;a href=&#34;http://cta.ornl.gov/transnet/CoFIPS00.txt&#34;&gt;CoFIPS00.txt&lt;/a&gt;. (MI,Washtenaw) turns into 26161. Feed a list of FIPS codes into the Census Reporter geo API, and out comes GeoJSON files, like so: &lt;a href=&#34;http://api.censusreporter.org/1.0/geo/show/tiger2013?geo_ids=05000US26103,05000US26003&#34;&gt;GeoJSON for Marquette and Alger Counties, Michigan&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;We have a list of counties for each utility, so at minimum we&#39;re set right there. But we need to get 2935 of them, and it&#39;s kind of slow to wait for each one to return before fetching the next one. That motivates the use of &lt;a href=&#34;http://www.gnu.org/software/parallel/&#34;&gt;GNU Parallel&lt;/a&gt;, a shell tool for executing jobs in parallel. On my four core MacBook Air I ran 32 jobs in parallel, and 2935 fetches (using &#34;curl&#34;) took 287 seconds with the median elapsed time for each job at about 3 seconds - about a 31x speedup vs doing things serially.&lt;/p&gt;

&lt;p&gt;However, the GeoJSON provided is really one shape per county, and we&#39;re looking for a complete service area in a single shape, so we need to merge the result together. Here a useful tool is the &#34;R&#34; stats package, particularly the &#34;rgeos&#34;, &#34;rgdal&#34;, and &#34;maptools&#34; packages. &lt;/p&gt;

&lt;p&gt;The first step is the &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgdal/docs/ogrInfo&#34;&gt;readOGR() function from the rgdal library&lt;/a&gt;. It takes a variety of input map formats including GeoJSON and shape files and pulls them into a Spatial vector object in R. rgdal is an R interface to Frank Warmerdam&#39;s &lt;a href=&#34;http://www.gdal.org/&#34;&gt;Geospatial Data Abstraction Library&lt;/a&gt;, and it handles both raster and vector geospatial data formats.&lt;/p&gt;

&lt;p&gt;Next, use the &lt;a href=&#34;http://www.inside-r.org/packages/cran/maptools/docs/unionSpatialPolygons&#34;&gt;unionSpatialPolygons() function from the maptools library&lt;/a&gt; to merge the county polygons into a single polygon. The docs say that this is a wrapper around the &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgeos/docs/gUnion&#34;&gt;gUnion() function from the rgeos library&lt;/a&gt;. rgeos comes from the Google Summer of Code 2010, and this &lt;a href=&#34;https://gsoc2010r.wordpress.com/2010/06/10/rgeos-introduction/&#34;&gt;introduction to the project&lt;/a&gt; explains the motivation. rgeos provides an R interface to &lt;a href=&#34;http://trac.osgeo.org/geos/&#34;&gt;GEOS&lt;/a&gt;, which is in turn a C++ port of &lt;a href=&#34;http://tsusiatsoftware.net/jts/main.html&#34;&gt;JTS&lt;/a&gt;, the Java Topology Suite.&lt;/p&gt;

&lt;p&gt;After you run the unionSpatialPolygons() operation, you will inevitably find that there are tiny holes in your unioned map. Geographic boundaries are complex, and as a result there are often little areas that don&#39;t quite touch, leaving little bits of land that aren&#39;t included anywhere. This algorithm from stackoverflow, &lt;a href=&#34;http://stackoverflow.com/questions/12663263/dissolve-holes-in-polygon-in-r&#34;&gt;Dissolve holes in polygon in R&lt;/a&gt;, takes care of the issue. &lt;/p&gt;

&lt;pre&gt;
G.rings = Filter(function(f){f@ringDir==1},G.union@polygons[[1]]@Polygons)
G.bounds = SpatialPolygons(list(Polygons(G.rings,ID=1)))
&lt;/pre&gt;

&lt;p&gt;As the Polygon documentation note for ringDir: &#34;the ring direction of the ring (polygon) coordinates, holes are expected to be anti-clockwise&#34;&lt;/p&gt;

&lt;p&gt;Having done this cleanup work, we return a GeoJSON file with the &lt;a href=&#34;http://www.rdocumentation.org/packages/leafletR/functions/toGeoJSON&#34;&gt;toGeoJSON() function from the leafletR package&lt;/a&gt;. Now I probably don&#39;t need to pull in all of leafletR; there&#39;s a &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgdal/docs/writeOGR&#34;&gt;writeOGR() function in the rgdal package&lt;/a&gt; that should also do the same work. The aside, though, is that &lt;a href=&#34;https://github.com/chgrl/leafletR&#34;&gt;leafletR&lt;/a&gt; creates nice maps in R using the &lt;a href=&#34;http://leafletjs.com/&#34;&gt;Leaflet&lt;/a&gt; Javascript library.&lt;/p&gt;

&lt;p&gt;With all that done, we have a new GeoJSON file that&#39;s a cleaned up merge of multiple county outlines done in R. However, the process isn&#39;t quite done. Some of the merged files don&#39;t have holes, but they do have &#34;slivers&#34;, little angular bits of geometry from where two counties don&#39;t quite touch at their corners. I didn&#39;t find an algorithm to unsliver in R, but this &lt;a href=&#34;http://gis.stackexchange.com/questions/120286/removing-small-polygons-gaps-in-a-shapely-polygon&#34;&gt;Removing small polygons gaps in a Shapely polygon&lt;/a&gt; did the trick. &lt;/p&gt;

&lt;pre&gt;
# Here&#39;s the algorithm
fx = poly.buffer(eps, 1, join_style=JOIN_STYLE.mitre).buffer(-eps, 1, join_style=JOIN_STYLE.mitre)
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://toblerity.org/shapely/manual.html&#34;&gt;Shapely&lt;/a&gt; from Sean Gilles is a &#34;Python package for set-theoretic analysis and manipulation of planar features using (via Pythonâ€™s ctypes module) functions from the well known and widely deployed GEOS library.&#34; So we&#39;re back to GEOS, which is also supported by R...which means I suspect this algorithm translates directly back into R by proper invocation of the equivalent &lt;a href=&#34;http://www.inside-r.org/packages/cran/rgeos/docs/gBuffer&#34;&gt;gBuffer() function from the rgeos package&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;No matter, I installed Shapely. It&#39;s pretty fast, and if I had time to refine this operation I&#39;d try to port all of the R code back into python using Shapely to see if I could get a meaningful speedup improvement.&lt;/p&gt;

&lt;p&gt;All this description! Here&#39;s the code.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/vielmetti/13f6feae14111ff1c148&#34;&gt;unsliver.py&lt;/a&gt; - in Python with the Shapely library&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/450495defeff6e4d1440&#34;&gt;mergeGeoJSON.Rscript&lt;/a&gt; - in R with rgeos, sp, rgdal, maptools, and leafletR&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And here&#39;s a picture:&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/vielmetti/2a6feadf268ca2b686e7.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Thanks go to Joe Germuska (for Census Reporter and csvkit), Markus Spath (for R help), Matt Hampel (for mapping help), Jacob Wasserman (for Shapely help), Stan Gregg (for outage mapping), Mohan Kartha (for GNU Parallel and AWS help), and anyone else who I missed along the way.                                          &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Merge the Ann Arbor Community Television Network with the Ann Arbor District Library&#39;s video services?</title>
      <link>http://vielmetti.github.io/post/2008/2008-12-31-ann-arbors-community-television-network-a-study-by-matt-hampel/</link>
      <pubDate>Wed, 31 Dec 2008 15:24:37 +0000</pubDate>
      
      <guid>http://vielmetti.github.io/post/2008/2008-12-31-ann-arbors-community-television-network-a-study-by-matt-hampel/</guid>
      <description>&lt;p&gt;Matt Hampel has released &lt;a href=&#34;http://matth.org/ctn/&#34;&gt;a study of Ann Arbor&amp;#39;s Community Television Network&lt;/a&gt;.&amp;#0160; He writes about an organization that was innovative and forward-looking in the 1970s that is now running with little public oversight and with only the most tentative ways of engaging with the public through networks other than cable television.&lt;/p&gt;
&lt;p&gt;Most notable in the whole discussion is &lt;a href=&#34;http://matth.org/ctn/?p=57&#34;&gt;a comparison of CTN&amp;#39;s video archiving and online access system with a similar setup at the Ann Arbor District Library&lt;/a&gt;.&amp;#0160; The library has direct access into the CTN digital network feeds, and uses off the shelf software to transcode video for delivery to cable.&amp;#0160; Where CTN is hamstrung by a reliance on city IT staff to do technology development - an IT staff that only does necessary maintenance - the AADL has an active IT department that is doing development in support of their mission.&lt;/p&gt;
&lt;p&gt;I&amp;#39;m sure that the people at CTN are doing a good job at their core mission, of teaching people how to do video production.&amp;#0160; The system is failing where it fails because there is not a corresponding core set of priority on video distribution and access beyond their cable television franchise.&amp;#0160; The whole system looks like it would be better off if CTN lopped off the approximately $180,000 per year they spend on City of Ann Arbor IT services and instead merged that effort into &lt;a href=&#34;http://www.aadl.org/video&#34;&gt;the Ann Arbor District Library&amp;#39;s existing video efforts.&amp;#0160; &lt;/a&gt;That would put both innovative production and innovative distribution under the same roof, and move citizens closer to access to &lt;a href=&#34;http://deoxy.org/seize_it.htm#p11&#34;&gt;public production libraries&lt;/a&gt; to let them be the media, not just consume it on channel 19.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>