<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Vacuum weblog from Edward Vielmetti</title>
    <link>http://vielmetti.github.io/post/</link>
    <description>Recent content in Posts on Vacuum weblog from Edward Vielmetti</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 20 Sep 2015 09:00:00 -0400</lastBuildDate>
    <atom:link href="http://vielmetti.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>AWS DynamoDB downtime, Sunday am, September 20, 2015</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-20-aws-dynamodb-downtime-sunday-am/</link>
      <pubDate>Sun, 20 Sep 2015 09:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-20-aws-dynamodb-downtime-sunday-am/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;A distributed system is one in which the failure of a computer
you didn&amp;rsquo;t even know existed can render your own computer unusable.
&lt;a href=&#34;http://research.microsoft.com/en-us/um/people/lamport/pubs/distributed-system.txt&#34;&gt;Leslie Lamport, 1987&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Amazon Web Services DynamoDB experienced downtime in the N Virginia
availability zone early Sunday morning, September 20, 2015. As
a result, a number of other AWS services inside N Virginia that
depend on DynamoDB also had downtime. Companies and organizations
that built services on top of those systems who didn&amp;rsquo;t have
geographic load balancing were having problems as well.&lt;/p&gt;

&lt;p&gt;Affected services include at least CloudWatch, SES, SNS, SQS, SWS,
AutoScale, Cloud Formation, Directory Service, Key Mgmt and Lambda,
according to a report on Hacker News.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://status.aws.amazon.com/&#34;&gt;http://status.aws.amazon.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Reports from &lt;a href=&#34;https://www.twitter.com/ylastic&#34;&gt;@ylastic&lt;/a&gt; on Twitter
have been helpful in keeping track of outages. &lt;a href=&#34;http://www.ylastic.com&#34;&gt;http://www.ylastic.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Down Detector has a page of AWS outages trouble reports, and
details pulled from Twitter. &lt;a href=&#34;https://downdetector.com/status/aws-amazon-web-services&#34;&gt;https://downdetector.com/status/aws-amazon-web-services&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When core infrastructure goes down, it tends to affect other platforms
that depend on that core infrastructure and that hide it from their
users. This in turn affects applications built on those platforms.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Docker: &amp;ldquo;We are currently seeing intermittent errors when pushing
and pulling, related to issues that AWS is having. We are currently
investigating the causes, and doing what we can to mitigate the
problems.&amp;rdquo; @dockerstatus and &lt;a href=&#34;http://status.docker.com/&#34;&gt;http://status.docker.com/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Heroku: &amp;ldquo;Starting new dynos (unidling, one-off, scaling or
restarting crashed apps, new releases) is still unavailable.&amp;rdquo; &amp;ldquo;Until
this incident is resolved, you might be unable to open new support
tickets with us. If you need to communicate with our support staff
during this time, please email outage-support@heroku.com.&amp;rdquo; @herokustatus
and &lt;a href=&#34;https://status.heroku.com/incidents/811&#34;&gt;https://status.heroku.com/incidents/811&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CircleCI: &amp;ldquo;Experiencing Issues with Heroku and AWS&amp;rdquo;. @circleci
and &lt;a href=&#34;http://status.circleci.com/&#34;&gt;http://status.circleci.com/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TravisCI: &amp;ldquo;Partial System Outage&amp;rdquo;. @traviscistatus and
&lt;a href=&#34;https://www.traviscistatus.com/&#34;&gt;https://www.traviscistatus.com/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are a lot of applications built on AWS and on Heroku,
which are at risk of downtime. A comprehensive list is probably
impossible, but here are some reports, in alphabetical order.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amazon Echo&lt;/li&gt;
&lt;li&gt;Amazon Instant Video&lt;/li&gt;
&lt;li&gt;AWS Support Tickets. Use this &lt;a href=&#34;http://www.amazon.com/gp/html-forms-controller/support-center-issues-u&#34;&gt;workaround&lt;/a&gt; ???&lt;/li&gt;
&lt;li&gt;Buffer&lt;/li&gt;
&lt;li&gt;Canva&lt;/li&gt;
&lt;li&gt;CoinSimple. &amp;ldquo;Widespread outage in Amazon Web Services is affecting several Internet services, including @CoinSimple. Please check here for updates.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;DuoLingo&lt;/li&gt;
&lt;li&gt;FastSpring. &amp;ldquo;We were having some intermittent problems due to #awsoutage. The problem is being resolved by Amazon.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;GroupMe&lt;/li&gt;
&lt;li&gt;HashiCorp. &amp;ldquo;Our infrastructure provider is experiencing issues which may cause our project websites to be inaccessible. Sorry for the inconvenience.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;IMDB&lt;/li&gt;
&lt;li&gt;Mediacom&lt;/li&gt;
&lt;li&gt;Medium. &amp;ldquo;Identified - We&amp;rsquo;re working to fix a major outage and will be back online as soon as possible.&amp;rdquo; &lt;a href=&#34;https://medium.statuspage.io/&#34;&gt;https://medium.statuspage.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Nest. &amp;ldquo;Weâ€™re investigating a service outage with the Nest mobile app and Cam services, and the team is working on a fix. Details to come.&amp;rdquo; &lt;a href=&#34;https://nest.com/support/#status&#34;&gt;https://nest.com/support/#status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Netflix. &amp;ldquo;We are currently experiencing issues streaming on all devices.
We are working to resolve the problem. We apologize for any inconvenience.&amp;rdquo; &lt;a href=&#34;https://help.netflix.com/help&#34;&gt;https://help.netflix.com/help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pocket&lt;/li&gt;
&lt;li&gt;Product Hunt.&lt;/li&gt;
&lt;li&gt;Social Flow&lt;/li&gt;
&lt;li&gt;Takipi. &amp;ldquo;Due to an AWS outage, we&amp;rsquo;re experiencing some slowness and connectivity issues. Stay tuned for updates&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Viber. &amp;ldquo;We are experiencing disruptions in our service. We are working to resolve this issue. Sorry for the inconvenience.&amp;rdquo; &lt;a href=&#34;https://support.viber.com/&#34;&gt;https://support.viber.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;waffle.io &amp;ldquo;Experiencing Issues with Heroku&amp;rdquo; &lt;a href=&#34;http://status.waffle.io/&#34;&gt;http://status.waffle.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wink.  &amp;ldquo;We are noticing increased connection issues for Wink users. Our engineers are working with Amazon to get this resolved ASAP.&amp;rdquo; &lt;a href=&#34;http://status.winkapp.com/&#34;&gt;http://status.winkapp.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Follow the discussion on Twitter on the #AWS hashtag, as well as
#awsoutage and #awsdown . Twitter is unaffected by this outage,
and Slack is also unaffected, which devops teams are both happy about.&lt;/p&gt;

&lt;h3 id=&#34;news-coverage:0bf1024f29774b00d48e1402990b91e9&#34;&gt;News coverage&lt;/h3&gt;

&lt;p&gt;This &lt;a href=&#34;https://news.ycombinator.com/item?id=10247307&#34;&gt;Hacker News discussion&lt;/a&gt;
is a good one as startups radio in their woes.&lt;/p&gt;

&lt;p&gt;TNW: &lt;a href=&#34;http://thenextweb.com/insider/2015/09/20/amazon-web-services-goes-down-taking-netflix-reddit-pocket-and-more-with-it/&#34;&gt;Amazon Web Services goes down, taking Netflix, Reddit, Pocket and more with it&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Into the matrix with Travis CI</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-16-travis-ci/</link>
      <pubDate>Wed, 16 Sep 2015 09:22:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-16-travis-ci/</guid>
      <description>

&lt;p&gt;Travis CI is a continuous integration tool, a description which means nothing
to people who have never used it. &amp;ldquo;Continuous integration&amp;rdquo; is one of these
industry phrases that doesn&amp;rsquo;t serve to illuminate the problem that it describes.&lt;/p&gt;

&lt;p&gt;Why would you care? Well, you&amp;rsquo;re working on a project with a bunch of other
people, and it&amp;rsquo;s kind of complicated. It might depend on some
&lt;a href=&#34;http://vielmetti.github.io/post/2015/2015-09-14-old-code-and-old-tools/&#34;&gt;old code&lt;/a&gt;
that&amp;rsquo;s fragile and should be tested all the time when it changes, or it might depend
some brand-spanking new code that doesn&amp;rsquo;t quite work and you know it
doesn&amp;rsquo;t work yet but you want the test system to be less tedious.&lt;/p&gt;

&lt;p&gt;What else looks like this?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://circleci.com/&#34;&gt;CircleCI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.solanolabs.com/&#34;&gt;Solano&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.appveyor.com/&#34;&gt;Appveyor&lt;/a&gt; (for Windows)&lt;/li&gt;
&lt;li&gt;Jenkins (self hosted)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What does this look like from a workflow perspective? Every time you check in a
file, the whole system builds, and all of the tests you have configured run.
With proper configuration you can build a matrix of versions or configurations
to test, so instead of a test running on a single system it can run on
3x3 or 2x4 or even 3x5x2 configurations.&lt;/p&gt;

&lt;h3 id=&#34;ruby:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Ruby&lt;/h3&gt;

&lt;p&gt;Travis CI&amp;rsquo;s default langauge is Ruby, and a lot of its tooling is written
in Ruby, so its Ruby support is quite good.&lt;/p&gt;

&lt;p&gt;Need a bunch of versions of Ruby? Use &lt;code&gt;rvm&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.travis-ci.com/user/languages/ruby/&#34;&gt;Building a Ruby Project, Travis CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;c-and-c:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;C and C++&lt;/h3&gt;

&lt;p&gt;The version of gcc and g++ on the default systems is 4.6, and you can
pull in 4.8 through &amp;lsquo;apt&amp;rsquo;, but you currently have to pick up 4.8 through
the &lt;code&gt;ubuntu-toolchain-r-test&lt;/code&gt; source and not simply by picking a gcc version.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rla/fast-feed/blob/master/.travis.yml&#34;&gt;rla/fast-feed/.travis.yml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;go-golang:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Go (golang)&lt;/h3&gt;

&lt;p&gt;Go (golang) has not gone through a lot of versions yet, and its latest 1.5
build is written in Go. Because the language is so young, there is not lots
of old code to port, and thus not too many version dependencies in libraries
to worry about yet.&lt;/p&gt;

&lt;p&gt;Testing two versions of Go (golang) plus Linux and Mac is easy, and the
configuration file doesn&amp;rsquo;t have to do anything funky.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/miekg/mmark/fast-feed/blob/master/.travis.yml&#34;&gt;miekg/mmark/.travis.yml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;nodejs:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;NodeJS&lt;/h3&gt;

&lt;p&gt;Node 4.0 is new, and the source currently of a fair bit of porting efforts as
a lot of things have broken not in the language but in its libraries. Fortunately,
the nodejs and iojs split has been reconciled, but unfortunately there&amp;rsquo;s about
six months of reunification work to be done.&lt;/p&gt;

&lt;p&gt;If you are porting or building to NodeJS 4.0, you have to drag in gcc 4.8. Previous
versions of gcc/g++ aren&amp;rsquo;t capable of coping with the C++11 constructs that are
in V8 and (especially) in &lt;code&gt;nan&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;NodeJS 4.0 on Mac? In addition to gcc 4.8, you have to drag in nvm.
The approachable way is to pull in the source from Github that lives
at &lt;code&gt;https://github.com/creationix/nvm.git&lt;/code&gt;, build it, and then use the
thing that you just built to subsequently pull in the right version of Node.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rla/fast-feed/blob/master/.travis.yml&#34;&gt;rla/fast-feed/.travis.yml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sass/node-sass/blob/master/.travis.yml&#34;&gt;sass/node-sass/.travis.yml&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;python:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Python&lt;/h3&gt;

&lt;p&gt;Python 2 and Python 3 are both available on Travis CI, including even
the latest Python 3.5.&lt;/p&gt;

&lt;p&gt;If you need three versions of Python plus three versions of Django:
use &amp;lsquo;env&amp;rsquo; to drive &amp;lsquo;pip&amp;rsquo;, and &amp;lsquo;matrix&amp;rsquo; to skip some builds where you
know that the combinations don&amp;rsquo;t work.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(datadesk/django-softhyphen)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lots and lots of Python versions, with different dependencies for
each version? The &lt;code&gt;tornado&lt;/code&gt; package has that.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(tornadoweb/tornado)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Looking to test Django plus Postgres?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;http://blog.schwuk.com/2014/06/13/using-travis-ci-for-testing-django-projects/&#34;&gt;http://blog.schwuk.com/2014/06/13/using-travis-ci-for-testing-django-projects/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;java:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Java&lt;/h3&gt;

&lt;p&gt;Java is supported.&lt;/p&gt;

&lt;p&gt;If you need Java 8 in a particular version, see the below issue on Github Issues.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/travis-ci/travis-ci/issues/4042&#34;&gt;Java 8 version update and bug&lt;/a&gt;,
Github issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;fortran:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Fortran&lt;/h3&gt;

&lt;p&gt;If you&amp;rsquo;re building scientific codes, Fortran is available through the &lt;code&gt;apt&lt;/code&gt;
mechanism.&lt;/p&gt;

&lt;h3 id=&#34;prolog:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Prolog&lt;/h3&gt;

&lt;p&gt;Prolog? Bring it in with apt-add-repository and apt-get. This is illustrative
of Travis CI&amp;rsquo;s ability to incorporate nearly any language, since you can install
new compilers and interpreters via package systems and then run them.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(rla/simple-template)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;multiple-langauges:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Multiple langauges&lt;/h3&gt;

&lt;p&gt;Two versions of Go plus Python plus an external dependency that&amp;rsquo;s being flaky?
Use &lt;code&gt;language: python&lt;/code&gt; because it&amp;rsquo;s easier to install &lt;code&gt;go&lt;/code&gt; from package
managers than it is to drag in the whole complex python multi-version infrastructure
from scratch.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(miekg/mmark, in progress)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;os-x:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;OS X&lt;/h3&gt;

&lt;p&gt;Use &amp;lsquo;osx_image&amp;rsquo; to tweak the version of &amp;lsquo;osx&amp;rsquo; that you get.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;http://docs.travis-ci.com/user/osx-ci-environment/&#34;&gt;http://docs.travis-ci.com/user/osx-ci-environment/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;(&lt;a href=&#34;http://blog.travis-ci.com/2015-09-09-xcode7-gm/&#34;&gt;http://blog.travis-ci.com/2015-09-09-xcode7-gm/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;both-linux-and-mac-in-general:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Both Linux and Mac, in general&lt;/h3&gt;

&lt;p&gt;Go is easy. (miekg/mmark)&lt;/p&gt;

&lt;p&gt;Node requires bringing in nvm and gcc 4.8. (rla/fastfeed)&lt;/p&gt;

&lt;p&gt;Python &amp;hellip; (don&amp;rsquo;t know)&lt;/p&gt;

&lt;p&gt;Prolog &amp;hellip; (don&amp;rsquo;t know)&lt;/p&gt;

&lt;p&gt;Ruby &amp;hellip; (should work pretty well, but not tested)&lt;/p&gt;

&lt;h2 id=&#34;acknowledgements:3a8a8bf475b0a53c2eff0d60f4a1b9bf&#34;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thanks to the following people for their help: Raivo Laanemets (@RaivoL),
Mohan Kartha (@mckartha), Chris Dzombak (@cdzombak), &amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zeno&#39;s Inbox</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-14-zenos-inbox/</link>
      <pubDate>Mon, 14 Sep 2015 01:15:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-14-zenos-inbox/</guid>
      <description>&lt;p&gt;Zeno&amp;rsquo;s inbox: cut in half in an hour; it always takes an hour to
cut it in half, no matter how few messages there are in it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Old code and old tools</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-14-old-code-and-old-tools/</link>
      <pubDate>Mon, 14 Sep 2015 00:05:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-14-old-code-and-old-tools/</guid>
      <description>&lt;p&gt;One of the problems with old codebases is that they are missing
modern tooling for automated builds, automated testing, and the
like.&lt;/p&gt;

&lt;p&gt;A second problem with old code is that the maintainers (absent
modern tooling) may not know or recognize what dependencies they
have.&lt;/p&gt;

&lt;p&gt;If an old package is not mostly self-contained, it&amp;rsquo;s likely to
have dependencies on other old code. This makes it hard to build,
as those older dependencies have either gone offline or have mutated
so that the old package doesn&amp;rsquo;t work with the new releases.&lt;/p&gt;

&lt;p&gt;To confound the matter even more, the newer tools for automated
builds, automated tests and so forth have their own sets of complex
dependencies, so much so that unless you are around some team that
has already absorbed these systems that you are unlikely to grasp
all of their deeply interconnected complexities.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s easy to cascade into a deep chasm of interlocking dependencies,
yielding enough complexity in the build process to frustrated people
who really care more about the code than how it&amp;rsquo;s packaged for distribution.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;I&amp;rsquo;ve been working this weekend on &lt;code&gt;tidy-html5&lt;/code&gt;. It has a Makefile
that&amp;rsquo;s generated by CMake, and its documentation is built with
a combination of xsltproc and Doxygen. As dependencies go that&amp;rsquo;s
not too bad, but the problem starts because the codebase lay dormant
for a couple of years, and somewhere between three and six years
of net development and build tooling efforts have passed it by.&lt;/p&gt;

&lt;p&gt;There was no Dockerfile for the project, so I wrote one to see if I
could get it to build and could understand all of the build dependencies.
That took a dozen tries, but eventually something went through to
completion.&lt;/p&gt;

&lt;p&gt;The first thing I noted was that it not only depended on CMake but
also a particularly relatively recent version of CMake. Ditto for
Doxygen - not any elderly Doxygen will do, but only a relatively
recent build. Doxygen has its own dependencies, lots of them - about
a gigabyte of code to download and install.&lt;/p&gt;

&lt;p&gt;Next was to pick up on a previous effort to build the whole thing under
Travis CI. Now I have a relatively old build system that doesn&amp;rsquo;t
have these old dependencies so I have to drag them in, and not only
drag in some version but a particularly new version.&lt;/p&gt;

&lt;p&gt;Dependencies satisfied, it&amp;rsquo;s time to build. I want this to work on my
Raspberry Pi, so I build it there, and discover that the version of
Doxygen there is too old (oops) so it doesn&amp;rsquo;t build the docs at all,
never mind the cheery message from the build script that claims
&amp;ldquo;TidyLib API documentation has been built&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Next, to test. Does it pass all of its tests? Well, there is a test
directory with a bunch of test cases, but the documentation says things
like&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Of course some of the tests were to say avoid a segfault found.
Other tests were to visually compare the original input test file
in a browser, with how the new output displayed in a browser. This
is a purely VISUAL compare, and can not be done in code.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Frankly, a test that can&amp;rsquo;t be done in code is one that I can&amp;rsquo;t run
in finite time. I build all of the tests, diff them with the reference
directory, and hope that the differences (there are a few) aren&amp;rsquo;t fatal.&lt;/p&gt;

&lt;p&gt;You see where this is going&amp;hellip;&lt;/p&gt;

&lt;p&gt;Frankly, it&amp;rsquo;s hard to get all of the tooling right. Travis CI has all of
its own quirks, and you have to set up a build correctly to get correct
build completion results. Once the build happens, you want some sort of
test harness to take all of these test cases and turn them into red lights
and green lights on your screen, and I don&amp;rsquo;t have that yet. Building
binaries is yet another task - one project contributor put together a
Jenkins install to take on that task.  And so forth.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;The piece of this that I care about is twofold. &lt;code&gt;tidy&lt;/code&gt; as it stands now
is frozen in 2009, and that&amp;rsquo;s before HTML5. &lt;code&gt;tidy-html5&lt;/code&gt; is caught in flux,
with a successful project rescue to get it to build at all, but not yet
the full-on automation and test infrastructure that projects like &lt;code&gt;docker&lt;/code&gt;
or &lt;code&gt;node.js&lt;/code&gt; have that allow for ferocious parallel development with
reasonable test and integration coverage.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;tidy&lt;/code&gt; project is very old (20 yrs), and if it&amp;rsquo;s going to pass
a full suite of automated tests on every check-in for every platform,
that&amp;rsquo;s going to take some consolidated effort - both by people who
care about the code (but not so much how it&amp;rsquo;s packaged) and by people
who care about the packaging (so that they can use the code). Not
only does the code need to evolve, but the build tools around the
code need to move forward.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing up your tweets</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-14-writing-up-your-tweets/</link>
      <pubDate>Mon, 14 Sep 2015 00:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-14-writing-up-your-tweets/</guid>
      <description>&lt;p&gt;I use Twitter regularly, and will frequently post short things there
that reflect some 140 character slice of what I&amp;rsquo;m thinking about. That&amp;rsquo;s
very quick, but at some level deeply unsatisfying, because how much can
you put into 140 characters? Never quite enough.&lt;/p&gt;

&lt;p&gt;The challenge is to write up your tweets into something longer. Take
the quip and turn it into a lead paragraph, then expand the lead paragraph
into three or five or fifteen or fifty more. Explore at leisure, including
excursions into thinking that don&amp;rsquo;t lend themselves to quippiness.&lt;/p&gt;

&lt;p&gt;In retrospect some tweets would have better been unwritten. The process
of twittering out a stream of observations gets in the way of assembling
the longer context. We focus on the stream of things coming our way,
never taking time to take stock of the situation and make it make more sense.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bluetooth and bluez inside Docker</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-11-bluetooth-bluez-inside-docker/</link>
      <pubDate>Fri, 11 Sep 2015 17:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-11-bluetooth-bluez-inside-docker/</guid>
      <description>&lt;p&gt;If you have a Bluetooth device attached to a Linux system, it&amp;rsquo;s
not clear how to attach that device to a Docker container on that
same system.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bluez&lt;/code&gt;, the Bluetooth driver for Linux, uses the &lt;code&gt;D-Bus&lt;/code&gt; to communicate.
This is more complex than a FIFO or socket interface, and thus less
easy to obviously say how it should be mapped into the Docker processs
and file system space.&lt;/p&gt;

&lt;p&gt;The references below show several ways of not doing this task,
and hopefully at some point will get answers on how to do it.&lt;/p&gt;

&lt;p&gt;References for further reading:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/docker/docker/issues/16208&#34;&gt;Bluetooth socket can&amp;rsquo;t be opened inside container&lt;/a&gt;, Docker Github issues 16208&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/28868393/accessing-bluetooth-dongle-from-inside-docker&#34;&gt;Accessing Bluetooth dongle from inside Docker?&lt;/a&gt;, Stack Overflow.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>xml2json as part of a web parsing pipeline</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-10-xml2json/</link>
      <pubDate>Thu, 10 Sep 2015 15:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-10-xml2json/</guid>
      <description>&lt;p&gt;The problem, simply put. You have a messy, real world HTML page;
you want to parse it to pull out some key element of it, to pass
along to some other task; you want that parsing pipeline to be
as compact as possible, because you&amp;rsquo;re running on a small machine,
and you&amp;rsquo;re doing the task frequently, and because the page may
change out from under you at any time.&lt;/p&gt;

&lt;p&gt;Clearly this is not an optimal task to have, but you live with
the web you have, not with the web you want to have.&lt;/p&gt;

&lt;p&gt;My approach to this task is to assemble a pipeline of Unix tools
that are as simple as possible, and that each do one thing pretty
well, and that in combination are all well-refined enough that there
are few surprises.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl&lt;/code&gt; is the workhorse for fetching pages. The invocation
&lt;code&gt;curl -m 15 -s http://example.com&lt;/code&gt; pulls that page from the
net and feeds it to standard output, but times out after 15 seconds
so your pipeline doesn&amp;rsquo;t completely fail if a remote site is down.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tidy&lt;/code&gt; is the first thing I look at when doing data transformation.
&lt;code&gt;tidy -q -asxml 2&amp;gt;/dev/null&lt;/code&gt; takes HTML and converts it, quietly
and uncomplainingly, into XML. It&amp;rsquo;s predictable and pretty fast.
&lt;code&gt;tidy&lt;/code&gt; has been around since the dawn of the web, and it newly
has a &lt;a href=&#34;https://github.com/htacg/tidy-html5&#34;&gt;Github project&lt;/a&gt; and
a &lt;a href=&#34;http://www.html-tidy.org/&#34;&gt;shiny web page&lt;/a&gt; and a support
consortium, so if you want to build from source it&amp;rsquo;s readily possible.&lt;/p&gt;

&lt;p&gt;Given XML, convert it to JSON. Here there are quite a few choices,
depending on which language and which parser you want to start with,
and the dependency tree is deep. I am least satisfied with my alternatives
here, thus the motivation for this writeup. In alphabetical order by
language:&lt;/p&gt;

&lt;p&gt;There is no POSIX standard &lt;code&gt;xml2json&lt;/code&gt; command written in C, alas.&lt;/p&gt;

&lt;p&gt;In C++ there&amp;rsquo;s an &lt;code&gt;xml2json&lt;/code&gt; command and library from Cheedoong Ch&amp;rsquo;ng.
The &lt;a href=&#34;https://github.com/Cheedoong/xml2json&#34;&gt;Github project&lt;/a&gt; says&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;xml2json is the first carefully written C++ library that converts
XML document to JSON format. It&amp;rsquo;s already been used in the soft
subtitle cross-domain solution at the server-end of Tencent Video
(&lt;a href=&#34;http://v.qq.com&#34;&gt;http://v.qq.com&lt;/a&gt;) and its CDNs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In Dart, see Steve Hamblett&amp;rsquo;s &lt;code&gt;xml2json.dart&lt;/code&gt; library.
The &lt;a href=&#34;https://github.com/shamblett/xml2json&#34;&gt;Github project&lt;/a&gt; includes
a set of unit tests, as well as explicit support for three
conventions (Parker, Badgerfish, and Google Data) for doing
the conversion. The test suites are welcomed.&lt;/p&gt;

&lt;p&gt;In Go, see Darren Elwood (textnode)&amp;rsquo;s &lt;code&gt;xml2json.go&lt;/code&gt;. The
&lt;a href=&#34;https://github.com/textnode/xml2json&#34;&gt;Github project&lt;/a&gt; includes
sample code to parse both RSS and generic XML files.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Transform a stream of XML into a stream of JSON, without requiring
a schema or structs, written in Go (golang.org)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In Node.JS, see &lt;code&gt;node-xml2json&lt;/code&gt; from BugLabs which
converts XML to JSON using &lt;code&gt;node-expat&lt;/code&gt;. You can install it
with &lt;code&gt;npm install xml2json&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In Perl, see Isidro Vila Verde&amp;rsquo;s &lt;code&gt;xml2json.pl&lt;/code&gt;, which has a
&lt;a href=&#34;https://github.com/jvverde/xml2json&#34;&gt;Github project&lt;/a&gt; and is based
on &lt;code&gt;XSLT&lt;/code&gt;. &amp;ldquo;You may need to install some perl modules before using it&amp;rdquo;
is the extent of the install instructions.&lt;/p&gt;

&lt;p&gt;In Python, see the &lt;code&gt;xmlutils&lt;/code&gt; package from Kailash Nadh
and Yigal Lazarev. You can
install it with &lt;code&gt;pip install xmlutils&lt;/code&gt;, or look at the
&lt;a href=&#34;https://github.com/knadh/xmlutils.py&#34;&gt;Github project&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;xmlutils.py is a set of Python utilities for processing xml files
serially for converting them to various formats (SQL, CSV, JSON).
The scripts use ElementTree.iterparse() to iterate through nodes
in an XML document, thus not needing to load the entire DOM into
memory. The scripts can be used to churn through large XML files
(albeit taking long :P) without memory hiccups.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once the original HTML document emerges nicely formatted as JSON,
it&amp;rsquo;s relatively easy to pick out elements from it in most cases.
Two tools to do this are &lt;code&gt;jq&lt;/code&gt; and &lt;code&gt;jp&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;jq&lt;/code&gt;, from Stephen Dolan, is a command-line JSON processor. Now
at version 1.5, it has a &lt;a href=&#34;https://stedolan.github.io/jq/&#34;&gt;web site&lt;/a&gt;
and a &lt;a href=&#34;https://github.com/stedolan/jq&#34;&gt;Github repository&lt;/a&gt;, plus
a handy &lt;a href=&#34;https://jqplay.org/&#34;&gt;online test site&lt;/a&gt; that lets you
do interactive tests. It&amp;rsquo;s written in portable C.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;jq is like sed for JSON data - you can use it to slice and filter
and map and transform structured data with the same ease that sed,
awk, grep and friends let you play with text.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;jp&lt;/code&gt;, from James Saryerwinnie, is a command line version of the JMESPath
query language for JSON, documented at &lt;a href=&#34;http://jmespath.org/&#34;&gt;http://jmespath.org/&lt;/a&gt; . This
is the same query language embedded into the Amazon Web Services
command line (AWS CLI), and it has powerful and compact operators
for extracting elements from a JSON document. There are libraries
in Python, PHP, Javascript, Ruby, Lua, and Go that implement JMESPath.
&lt;code&gt;jp&lt;/code&gt; is written in Go, and has cross-development tools available.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;So that&amp;rsquo;s the environment. It&amp;rsquo;s a bit of a pain to get all of
those tools running on a new bare-metal machine, so my thought
is to put everything into one Docker build and make it straightforward
to wrap everything together. Ideally this will be a fairly
minimalist build, and thus the challenge is to find the set
of dependencies that is small enough.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing a Raspberry Pi OpenVPN with Salt</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-07-managing-raspberry-pi-openvpn-with-salt/</link>
      <pubDate>Mon, 07 Sep 2015 00:45:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-07-managing-raspberry-pi-openvpn-with-salt/</guid>
      <description>&lt;p&gt;When you successfully install something complex, and then you fail
to document one small thing about how you did it, the inclination is
to over-document once you get it working again; thus, this expository
account of managing a set of Raspberry Pi devices using the &amp;ldquo;salt&amp;rdquo;
system. It&amp;rsquo;s not a complete start to finish writeup, just a pointer to
the bits that I had forgotten.&lt;/p&gt;

&lt;p&gt;This is mostly experimental at this point, and the collection of devices
is relatively small. I am pretty convinced that the Pi 2 has plenty of
gas to do more things than most people will try to do, but the original
Pi is slow enough and limited enough to perhaps make this scheme impractical.&lt;/p&gt;

&lt;p&gt;When it works, you have a set of Pi&amp;rsquo;s in various locations, all networked
together into an OpenVPN configuration.&lt;/p&gt;

&lt;p&gt;First to read is &lt;a href=&#34;http://garthwaite.org/virtually-secure-with-openvpn-pillars-and-salt.html&#34;&gt;Virtually secure with openvpn, pillars, and salt&lt;/a&gt; which
goes over the structure and motivation and design for an OpenVPN
network that answers this problem:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How do I use salt to create and install the openvpn and client specific config files for each minion &amp;ndash;on demand?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you follow Dan Garthwaite&amp;rsquo;s design you get a reasonable
approach - when you bring up a new client you get a new key
generated, and on the server you run &lt;code&gt;salt-key -a&lt;/code&gt; to accept the
new one.&lt;/p&gt;

&lt;p&gt;This leaves only the problem of bootstrapping on the Pi&amp;rsquo;s themselves.
The version of &amp;lsquo;salt-minion&amp;rsquo; that&amp;rsquo;s available from the default
Debian system is very ancient, so that&amp;rsquo;s useless. What you want
can be found at
&lt;a href=&#34;http://servernetworktech.com/2014/05/setup-debian-saltstack-minion-single-command/&#34;&gt;Setup a Debian Saltstack minion with a single command&lt;/a&gt;
on the Server Network Tech blog; it brings in a new Debian
source list from Salt so that the bootstrap is easier.&lt;/p&gt;

&lt;p&gt;(I realize this isn&amp;rsquo;t a start to finish tutorial, just a few points
of clarification for bringing up new nodes once you have the whole
Salt environment running; sorry about that.)&lt;/p&gt;

&lt;p&gt;The one thing I have left to do to make the setup more airtight is
to automate the process of locking down the sshd_config so that &amp;ldquo;root&amp;rdquo;
and &amp;ldquo;pi&amp;rdquo; can&amp;rsquo;t login from far away, so that I can keep their default
passwords but restrict access to console access only.&lt;/p&gt;

&lt;p&gt;Not mentioned here is that these Pi configuraiton are all on top of
the Hypriot setup so that they can (and do) run Docker. Though Docker
on the original Pi with salt-minion running doesn&amp;rsquo;t leave a ton of
space spare for actual work to be done.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep web</title>
      <link>http://vielmetti.github.io/post/2015/2015-09-04-deep-web/</link>
      <pubDate>Fri, 04 Sep 2015 08:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-09-04-deep-web/</guid>
      <description>&lt;p&gt;I think this whole &amp;ldquo;deep web&amp;rdquo; thing is mis-stated for the most part.&lt;/p&gt;

&lt;p&gt;Since forever there have been Internet-based services that have
been unreachable to the public unless you had some kind of specialized
access. Private internal networks are the norm at companies of any
size. Calling that the &amp;ldquo;hidden web&amp;rdquo;, &amp;ldquo;deep web&amp;rdquo; is non-sensical in
that it&amp;rsquo;s not really hidden to anyone who has proper access to it
- it&amp;rsquo;s just that you (and Google) don&amp;rsquo;t and won&amp;rsquo;t have that access.&lt;/p&gt;

&lt;p&gt;Furthermore there are enormous numbers of home networks behind NAT
devices where there are home servers that are either accidentally
or deliberately off-limits. If you want to get that data onto the
public network you actually have to do some work to make it happen.&lt;/p&gt;

&lt;p&gt;I even have a batch of test services running on my laptop that don&amp;rsquo;t
have any public presence at all beyond my keyboard.&lt;/p&gt;

&lt;p&gt;There happen to be parts of the &amp;ldquo;secret web&amp;rdquo;, &amp;ldquo;deep web&amp;rdquo; etc where
there&amp;rsquo;s some naughty content, but for the most part the Internet
architecture makes it completely normal to have hosts that only
have partial routing to the world, and most of the time that&amp;rsquo;s
completely innocuous.&lt;/p&gt;

&lt;p&gt;(As originally posted to Quora to answer the question, &amp;ldquo;How is the
Deep Web so huge?)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Over the top networks, a history of building new systems on the wreckage of the old</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-30-over-the-top-networks/</link>
      <pubDate>Sun, 30 Aug 2015 14:00:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-30-over-the-top-networks/</guid>
      <description>&lt;p&gt;This is a copy of a proposal for a talk at the Radical/Networks conference.
&lt;a href=&#34;https://github.com/chootka/radical-networks/issues/27&#34;&gt;The original proposal&lt;/a&gt;
is on Github.
The event runs October 24th and 25th (Saturday +
Sunday) from 10a - 7p at NYU Poly in Brooklyn, NY.&lt;/p&gt;

&lt;p&gt;Title: Over the top networks, a history of building new systems on the wreckage of the old&lt;/p&gt;

&lt;p&gt;New networks often feed off of old ones, in ways that characteristically
draw energy from aspects of those networks that are easy to take
over and hard to defend. In this talk, I&amp;rsquo;ll look at the waves of
creative destruction that are unleashed when network developers
find existing infrastructure that can be exploited for new ends,
and the ways that commoditized networks fight back to avoid being
turned into dumb pipes.&lt;/p&gt;

&lt;p&gt;The talk will look at the history and the future of these overlay
or over-the-top networks, going back three decades for stories of
networks like Usenet, electronic mail, and payments networks and
how they started out parasitizing existing older networks only be
overtopped by other interests as file sharing, security, and identity
layers of newer networks. I&amp;rsquo;ll look at rules for developers of
radical advances in networks and guidelines to avoid the pitfalls
of unexpected dependencies and hidden traps.&lt;/p&gt;

&lt;p&gt;Edward Vielmetti has been working on the Internet since 1985 from
Ann Arbor, Michigan. His background includes work on the early
commercialization and privatization of the Internet. Previous
conference presentations include &amp;ldquo;WYSSA means all my love, darling:
A social history of the Internet from the carrier pigeon to Antarctic
morse code&amp;rdquo; (UPA 2006) and &amp;ldquo;Perils and Pitfalls of Practical
Cybercommerce&amp;rdquo; (ARABANK 1996, Dubai, United Arab Emirates).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terabytes, at least in theory</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-29-terabytes-at-least-in-theory/</link>
      <pubDate>Sat, 29 Aug 2015 21:30:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-29-terabytes-at-least-in-theory/</guid>
      <description>&lt;p&gt;I have a new terabyte drive. It&amp;rsquo;s impossibly small, impossibly
cheap, and just fast enough to accomplish astounding backup tasks
in what seems like not enough time.&lt;/p&gt;

&lt;p&gt;It is, however, a mass storage device that depends on rotating
magnetic media, and as such it&amp;rsquo;s obsolete.&lt;/p&gt;

&lt;p&gt;The drive cost less than $50. For about 10 times that right now,
I can buy another terabyte drive that is solid state rather than
spinning oxide storage. The bet that I have is that in three years,
the cost of a terabyte of SSD drive will have come down so far -
perhaps to $100 - that no one will have any interest in selling me
a 1T drive that isn&amp;rsquo;t SSD.&lt;/p&gt;

&lt;p&gt;My frame of reference for &amp;ldquo;what does har drive storage cost&amp;rdquo; dates
back as far as the 10 megabyte, $1000 drive, which I really wanted
and which I never bought for my own self. And then back a little bit
further, when a 5 megabyte Davong drive was the bee&amp;rsquo;s knees.&lt;/p&gt;

&lt;p&gt;What do you do with a terabyte? Lots of backups.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RRDTool v1.5.4 on Raspberry Pi</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-26-rrdtool-on-raspberry-pi/</link>
      <pubDate>Wed, 26 Aug 2015 17:15:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-26-rrdtool-on-raspberry-pi/</guid>
      <description>&lt;p&gt;The second in a series of efforts to get a suitable database for
the task.&lt;/p&gt;

&lt;p&gt;The challenge is that there&amp;rsquo;s a lot of data out there that changes
all the time, and you want to gather it, make sure it&amp;rsquo;s sane, squirrel
it away for later analysis, put together a pretty chart so that you
can spot check it, feed it into other systems not yet fully dreamed
of to cause actions, and so on. It&amp;rsquo;s the &amp;ldquo;big data&amp;rdquo; problem in that
you have sensors that throw off a lot of data exhaust, and it&amp;rsquo;s the
&amp;ldquo;internet of things&amp;rdquo; problem because mostly you&amp;rsquo;re looking at things
with data and not say imagery or lots of freeform text.&lt;/p&gt;

&lt;p&gt;Phase one of this effort was wrestling MongoDB into service, which
ended up being only a handful of lines of code; but the concern expressed
was the Mongo though fun and easy for small projects gets messy for
big ones, and there&amp;rsquo;s good reason not to succumb to its siren song.
The other reason to stop using Mongo when I did was that it really
doesn&amp;rsquo;t love the Raspberry Pi architecture. Since there were only really
a very few lines of code, and since that code works on my Mac, no time
lost - but time to move on.&lt;/p&gt;

&lt;p&gt;Phase two is a look at RRDtool, a &amp;ldquo;round robin&amp;rdquo; database very often used
for network management efforts. One of the nice things about a round robin
database is that the database itself is at its maximum size when you
create it, and it doesn&amp;rsquo;t grow - rather, it cycles out the old data
at whatever time resolution you want to get rid of it. With that in
mind you can have a single database with 10 years worth of daily averages
and a whole year of minute-by-minute data and it hardly takes up any
room at all - perfect for the sort of &amp;ldquo;data logger&amp;rdquo; applications that
I find myself in the middle of.&lt;/p&gt;

&lt;p&gt;If you do &lt;code&gt;apt-get install rrdtool&lt;/code&gt; on the Pi you get a working
copy of RRDtool 1.4.7, compiled in 2012. That&amp;rsquo;s pretty long in the
tooth, so I decided to pull down v1.5.4 from the
&lt;a href=&#34;http://oss.oetiker.ch/rrdtool/pub/?M=D&#34;&gt;RRDtool download area&lt;/a&gt;. The release notes note
dependencies as follows&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get install libpango1.0-dev libxml2-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and once you have those pulled down the build is not fast but not
difficult.&lt;/p&gt;

&lt;p&gt;Pushing data into RRDtool is as simple as a shell script run out of
cron. The first data point I&amp;rsquo;m graphing is &amp;ldquo;total spaces free in the
Ann Arbor DDA parking garages&amp;rdquo;, and I&amp;rsquo;m collecting that on two machines -
my Mac when it&amp;rsquo;s running, and the Pi at all times. A second graph
I want to capture is some really basic weather information, and a
third is inside-the-machine environmental sensing starting with the
system temperature of the Pi.&lt;/p&gt;

&lt;p&gt;It was really useful to make the first round of this be Mongo, because
with Mongo you really don&amp;rsquo;t have to decide anything when you set things up -
just dump in raw JSON documents and hope for the best.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://vielmetti.github.io/images/parking.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;RRDtool graph of available parking in downtown Ann Arbor&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First pass at MongoDB</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-26-first-pass-at-mongodb/</link>
      <pubDate>Wed, 26 Aug 2015 00:40:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-26-first-pass-at-mongodb/</guid>
      <description>&lt;p&gt;Somehow I never learned SQL along the way. At various times I have
learned enough of the database lingo (inner join, outer join etc) to
know where I missing things, and I have a lot of experience with common
Unix tools (grep, sed, awk) and some newfangled tools (jq, csvkit) to
be able to turn a big pile of data into a smaller pile of data. But
I don&amp;rsquo;t have database chops per se right now, and sometimes that&amp;rsquo;s a
limiting factor on projects when the data sets are too big.&lt;/p&gt;

&lt;p&gt;JSON formatted data seems to have won the data serialization wars, so
that at least transforms the problem on input; with jq (now in release 1.5)
you can take a stream of JSON formatted data that&amp;rsquo;s almost what you want
and mangle it until it&amp;rsquo;s really what you want. Lots of web APIs spit out
some flavor of JSON, and even if it&amp;rsquo;s not precisely formatted to be ready
for input into something else, at least there&amp;rsquo;s a straightforward way to
munge it into something better.&lt;/p&gt;

&lt;p&gt;That leaves the task of dumping these JSON blobs into a database, in
such a way that you can relatively easily get something back out of the
database that answers what you are really trying to answer again. Here
the world splits into two pieces: the SQL databases that have been taught
how to deal with JSON (specifically PostgreSQL, as of version 9.3), and the
so-called NoSQL databases (specifically MongoDB).&lt;/p&gt;

&lt;p&gt;Even Mongo knows that &lt;a href=&#34;http://blog.mongodb.org/post/62899600960/scaling-advice-from-mongohq&#34;&gt;scaling MongoDB is hard&lt;/a&gt;, though
they frame it as the first 100 gigabytes of data stored in the system is
easy. Fortunately, the first set of tasks I have to throw at this particular
problem space are relatively small, databases measured in the 10s of megabytes.
Pretty much anything will work - even grep - on this size data. The
challenge becomes more of a how fast can you get something real running
that has relatively simple queries, relatively performant results, and
that can run on inexpensive hardware.&lt;/p&gt;

&lt;p&gt;So I learned just enough Mongo to handle one tiny task: connect to an API,
download some data periodically, clean it up enough so that it&amp;rsquo;s regular,
and store it away; and then construct queries that are conceptually simple
but that answer real questions. From there, do the ad hoc queries that prove
to yourself that you have the right data, and then bootstrap some web
app on top of all that to show off what you&amp;rsquo;ve done.&lt;/p&gt;

&lt;p&gt;The results here accomplish a relatively complex task with a handful of
lines of code. An API from a local agency is queried that has real time data.
That data gets cleaned up and standardized and then inserted into a Mongo
database. It&amp;rsquo;s all of 6 lines of code.&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/vielmetti/85d5fba07b98e552c912.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Listening to Radio Havana Cuba via a shortwave software defined radio</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-24-radio-havana-cuba/</link>
      <pubDate>Mon, 24 Aug 2015 22:45:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-24-radio-havana-cuba/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m listening to Radio Havana Cuba via a shortwave listening post set up
with a software defined radio. The system, &lt;a href=&#34;http://blerp2.dyndns.org:8901/&#34;&gt;Great Lakes Listening Post&lt;/a&gt;,
has four separate radios covering the 80, 40, 30, and 20 meter bands, all
fed into the &lt;a href=&#34;http://www.websdr.org&#34;&gt;WebSDR&lt;/a&gt; software from PA3FWM. It&amp;rsquo;s
located in Michigan, and thus approximates the radio reception I&amp;rsquo;d get from
my own receiver.&lt;/p&gt;

&lt;p&gt;R Havana Cuba is a good station to listen to. There&amp;rsquo;s interesting political
commentary, fun music, and the occasional show about shortwave listening.
The signal is usually strong and the frequency (6000 kHz) is easy to remember.
Most international braodcasters have refocused their attention on Internet
based broadcasts, but R Havana Cuba continues on with its shortwave service.
My Sony ICF-2010 set from the 1980s still has a Cold War era present to that
frequency.&lt;/p&gt;

&lt;p&gt;The biggest advantage to web-based software radio is the ability to pick up
stations from listening posts around the world. The WebSDR platform has stations
with good antennas around the world, and they each allow multiple listeners
to tune in to different stations without any of them monopolizing the receiver.
Most web-based SDR systems focus on shortwave and HF amateur bands, but every
so often you find a system that can tune domestic medium wave (AM) and
long wave bands, which gives you a chance to hear programs that were
never anticipated to be listened to from half way around the world.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://vielmetti.github.io/images/R-Havana-Cuba-WebSDR.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Radio Havana Cuba on Great Lakes Listening Post WebSDR&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Discussion on &lt;a href=&#34;https://www.facebook.com/edward.vielmetti/posts/10105619636684853&#34;&gt;Facebook&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer dinner</title>
      <link>http://vielmetti.github.io/post/2015/2015-08-24-summer-dinner/</link>
      <pubDate>Mon, 24 Aug 2015 11:55:00 -0400</pubDate>
      
      <guid>http://vielmetti.github.io/post/2015/2015-08-24-summer-dinner/</guid>
      <description>&lt;p&gt;Dinner: roasted red peppers, roasted green beans, boiled potatoes,
cool and crunchy red pepper and yellow tomato salad with balsamic,
kale, and either eggs or bratwurst depending on your preferences.
I really prefer cooking in the summertime!&lt;/p&gt;

&lt;p&gt;Discussion on &lt;a href=&#34;https://www.facebook.com/edward.vielmetti/posts/10105614819932663?notif_t=like&#34;&gt;Facebook&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>